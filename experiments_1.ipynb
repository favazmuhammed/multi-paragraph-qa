{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"hUYhmftnUcEy"},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"code","source":["!pip install allennlp"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B6yQv9uosAn-","executionInfo":{"status":"ok","timestamp":1674898023261,"user_tz":-330,"elapsed":207382,"user":{"displayName":"Chitresh Kulhade","userId":"02463847632592786086"}},"outputId":"89819fe5-74a3-4cd0-b53c-e5b23e68547d"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting allennlp\n","  Downloading allennlp-2.10.1-py3-none-any.whl (730 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m730.2/730.2 KB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting wandb<0.13.0,>=0.10.0\n","  Downloading wandb-0.12.21-py2.py3-none-any.whl (1.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting filelock<3.8,>=3.3\n","  Downloading filelock-3.7.1-py3-none-any.whl (10 kB)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting requests>=2.28\n","  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 KB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting spacy<3.4,>=2.1.0\n","  Downloading spacy-3.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typer>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from allennlp) (0.7.0)\n","Collecting termcolor==1.1.0\n","  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting sentencepiece>=0.1.96\n","  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pytest>=6.2.5\n","  Downloading pytest-7.2.1-py3-none-any.whl (317 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 KB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting cached-path<1.2.0,>=1.1.3\n","  Downloading cached_path-1.1.6-py3-none-any.whl (26 kB)\n","Requirement already satisfied: dill>=0.3.4 in /usr/local/lib/python3.8/dist-packages (from allennlp) (0.3.6)\n","Collecting fairscale==0.4.6\n","  Downloading fairscale-0.4.6.tar.gz (248 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.2/248.2 KB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: nltk>=3.6.5 in /usr/local/lib/python3.8/dist-packages (from allennlp) (3.7)\n","Requirement already satisfied: traitlets>5.1.1 in /usr/local/lib/python3.8/dist-packages (from allennlp) (5.7.1)\n","Requirement already satisfied: protobuf<4.0.0,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from allennlp) (3.19.6)\n","Requirement already satisfied: scikit-learn>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from allennlp) (1.0.2)\n","Collecting lmdb>=1.2.1\n","  Downloading lmdb-1.4.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (306 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m306.1/306.1 KB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub>=0.0.16\n","  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting h5py>=3.6.0\n","  Downloading h5py-3.8.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.62 in /usr/local/lib/python3.8/dist-packages (from allennlp) (4.64.1)\n","Collecting transformers<4.21,>=4.1\n","  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m104.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: more-itertools>=8.12.0 in /usr/local/lib/python3.8/dist-packages (from allennlp) (9.0.0)\n","Collecting torch<1.13.0,>=1.10.0\n","  Downloading torch-1.12.1-cp38-cp38-manylinux1_x86_64.whl (776.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorboardX>=1.2\n","  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.4/125.4 KB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torchvision<0.14.0,>=0.8.1\n","  Downloading torchvision-0.13.1-cp38-cp38-manylinux1_x86_64.whl (19.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jsonnet>=0.10.0\n","  Downloading jsonnet-0.19.1.tar.gz (593 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m593.6/593.6 KB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.21.4 in /usr/local/lib/python3.8/dist-packages (from allennlp) (1.21.6)\n","Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.8/dist-packages (from allennlp) (1.7.3)\n","Collecting base58>=2.1.1\n","  Downloading base58-2.1.1-py3-none-any.whl (5.6 kB)\n","Collecting boto3<2.0,>=1.0\n","  Downloading boto3-1.26.59-py3-none-any.whl (132 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 KB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: google-cloud-storage<3.0,>=1.32.0 in /usr/local/lib/python3.8/dist-packages (from cached-path<1.2.0,>=1.1.3->allennlp) (2.7.0)\n","Collecting huggingface-hub>=0.0.16\n","  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.5/163.5 KB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting rich<13.0,>=12.1\n","  Downloading rich-12.6.0-py3-none-any.whl (237 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.5/237.5 KB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.0.16->allennlp) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.0.16->allennlp) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.0.16->allennlp) (4.4.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk>=3.6.5->allennlp) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk>=3.6.5->allennlp) (1.2.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk>=3.6.5->allennlp) (2022.6.2)\n","Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from pytest>=6.2.5->allennlp) (2.0.1)\n","Collecting pluggy<2.0,>=0.12\n","  Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n","Collecting exceptiongroup>=1.0.0rc8\n","  Downloading exceptiongroup-1.1.0-py3-none-any.whl (14 kB)\n","Collecting iniconfig\n","  Downloading iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\n","Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.8/dist-packages (from pytest>=6.2.5->allennlp) (22.2.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.28->allennlp) (1.24.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.28->allennlp) (2.10)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.28->allennlp) (2.1.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.28->allennlp) (2022.12.7)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=1.0.1->allennlp) (3.1.0)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (6.3.0)\n","Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n","  Downloading pydantic-1.8.2-cp38-cp38-manylinux2014_x86_64.whl (13.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.0.8)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.0.11)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (0.10.1)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (1.0.4)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (1.0.9)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (57.4.0)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.3.0)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (0.10.1)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (2.4.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (2.11.3)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (2.0.8)\n","Collecting typer>=0.4.1\n","  Downloading typer-0.4.2-py3-none-any.whl (27 kB)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (2.0.7)\n","Collecting thinc<8.1.0,>=8.0.14\n","  Downloading thinc-8.0.17-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (671 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m671.1/671.1 KB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (0.7.9)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision<0.14.0,>=0.8.1->allennlp) (7.1.2)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sentry-sdk>=1.0.0\n","  Downloading sentry_sdk-1.14.0-py2.py3-none-any.whl (178 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.9/178.9 KB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.8/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (2.3)\n","Collecting docker-pycreds>=0.4.0\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.8/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (1.15.0)\n","Collecting shortuuid>=0.5.0\n","  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n","Collecting setproctitle\n","  Downloading setproctitle-1.3.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n","Collecting pathtools\n","  Downloading pathtools-0.1.2.tar.gz (11 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (5.4.8)\n","Collecting GitPython>=1.0.0\n","  Downloading GitPython-3.1.30-py3-none-any.whl (184 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.0/184.0 KB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Collecting s3transfer<0.7.0,>=0.6.0\n","  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 KB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting botocore<1.30.0,>=1.29.59\n","  Downloading botocore-1.29.59-py3-none-any.whl (10.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m106.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting gitdb<5,>=4.0.1\n","  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.4.0)\n","Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.16.0)\n","Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.8/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.11.0)\n","Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.3.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.9->huggingface-hub>=0.0.16->allennlp) (3.0.9)\n","Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.8/dist-packages (from rich<13.0,>=12.1->cached-path<1.2.0,>=1.1.3->allennlp) (2.6.1)\n","Collecting commonmark<0.10.0,>=0.9.0\n","  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 KB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting urllib3<1.27,>=1.21.1\n","  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.4,>=2.1.0->allennlp) (2.0.1)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.8/dist-packages (from botocore<1.30.0,>=1.29.59->boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.8.2)\n","Collecting smmap<6,>=3.0.1\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.8/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.58.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (5.2.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (4.9)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (0.2.8)\n","Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.8/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.5.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (0.4.8)\n","Building wheels for collected packages: fairscale, termcolor, jsonnet, sacremoses, pathtools\n","  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fairscale: filename=fairscale-0.4.6-py3-none-any.whl size=307251 sha256=f5340b73070eed43c936cb190bbdffdfb0f1cf6b1384645142a5376410688a0b\n","  Stored in directory: /root/.cache/pip/wheels/77/4c/a4/f6c0eec2ec5c8ffca075e62b0329801f862e1f1b71422f456b\n","  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4849 sha256=2158d82b3b4c55e1748daa1f5c809cb7996fa96641b5b73305724232617cc84e\n","  Stored in directory: /root/.cache/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n","  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for jsonnet: filename=jsonnet-0.19.1-cp38-cp38-linux_x86_64.whl size=6332960 sha256=9adc7bf355545a54e37e8a2c9c6cb9c4a2ae5d73e4cc60bff6139838d8fcf7ec\n","  Stored in directory: /root/.cache/pip/wheels/64/ec/56/de861aae102c449ade2378772abbf9eb7e9acfe9a80f3e6036\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=6dfb5a4787dc3c5d37da59544617aaead3ebd9a3317a7848a662fa5f6cbced59\n","  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=341e01879b397ce2ee8f34c8ec27e8ca5f18784e6c471fce3c579ffa9803d01e\n","  Stored in directory: /root/.cache/pip/wheels/4c/8e/7e/72fbc243e1aeecae64a96875432e70d4e92f3d2d18123be004\n","Successfully built fairscale termcolor jsonnet sacremoses pathtools\n","Installing collected packages: tokenizers, termcolor, sentencepiece, pathtools, lmdb, jsonnet, commonmark, urllib3, typer, torch, tensorboardX, smmap, shortuuid, setproctitle, sacremoses, rich, pydantic, pluggy, jmespath, iniconfig, h5py, filelock, exceptiongroup, docker-pycreds, base58, thinc, sentry-sdk, requests, pytest, gitdb, fairscale, botocore, torchvision, spacy, s3transfer, huggingface-hub, GitPython, wandb, transformers, boto3, cached-path, allennlp\n","  Attempting uninstall: termcolor\n","    Found existing installation: termcolor 2.2.0\n","    Uninstalling termcolor-2.2.0:\n","      Successfully uninstalled termcolor-2.2.0\n","  Attempting uninstall: lmdb\n","    Found existing installation: lmdb 0.99\n","    Uninstalling lmdb-0.99:\n","      Successfully uninstalled lmdb-0.99\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","  Attempting uninstall: typer\n","    Found existing installation: typer 0.7.0\n","    Uninstalling typer-0.7.0:\n","      Successfully uninstalled typer-0.7.0\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.13.1+cu116\n","    Uninstalling torch-1.13.1+cu116:\n","      Successfully uninstalled torch-1.13.1+cu116\n","  Attempting uninstall: pydantic\n","    Found existing installation: pydantic 1.10.4\n","    Uninstalling pydantic-1.10.4:\n","      Successfully uninstalled pydantic-1.10.4\n","  Attempting uninstall: pluggy\n","    Found existing installation: pluggy 0.7.1\n","    Uninstalling pluggy-0.7.1:\n","      Successfully uninstalled pluggy-0.7.1\n","  Attempting uninstall: h5py\n","    Found existing installation: h5py 3.1.0\n","    Uninstalling h5py-3.1.0:\n","      Successfully uninstalled h5py-3.1.0\n","  Attempting uninstall: filelock\n","    Found existing installation: filelock 3.9.0\n","    Uninstalling filelock-3.9.0:\n","      Successfully uninstalled filelock-3.9.0\n","  Attempting uninstall: thinc\n","    Found existing installation: thinc 8.1.6\n","    Uninstalling thinc-8.1.6:\n","      Successfully uninstalled thinc-8.1.6\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.25.1\n","    Uninstalling requests-2.25.1:\n","      Successfully uninstalled requests-2.25.1\n","  Attempting uninstall: pytest\n","    Found existing installation: pytest 3.6.4\n","    Uninstalling pytest-3.6.4:\n","      Successfully uninstalled pytest-3.6.4\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.14.1+cu116\n","    Uninstalling torchvision-0.14.1+cu116:\n","      Successfully uninstalled torchvision-0.14.1+cu116\n","  Attempting uninstall: spacy\n","    Found existing installation: spacy 3.4.4\n","    Uninstalling spacy-3.4.4:\n","      Successfully uninstalled spacy-3.4.4\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchtext 0.14.1 requires torch==1.13.1, but you have torch 1.12.1 which is incompatible.\n","torchaudio 0.13.1+cu116 requires torch==1.13.1, but you have torch 1.12.1 which is incompatible.\n","en-core-web-sm 3.4.1 requires spacy<3.5.0,>=3.4.0, but you have spacy 3.3.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed GitPython-3.1.30 allennlp-2.10.1 base58-2.1.1 boto3-1.26.59 botocore-1.29.59 cached-path-1.1.6 commonmark-0.9.1 docker-pycreds-0.4.0 exceptiongroup-1.1.0 fairscale-0.4.6 filelock-3.7.1 gitdb-4.0.10 h5py-3.8.0 huggingface-hub-0.10.1 iniconfig-2.0.0 jmespath-1.0.1 jsonnet-0.19.1 lmdb-1.4.0 pathtools-0.1.2 pluggy-1.0.0 pydantic-1.8.2 pytest-7.2.1 requests-2.28.2 rich-12.6.0 s3transfer-0.6.0 sacremoses-0.0.53 sentencepiece-0.1.97 sentry-sdk-1.14.0 setproctitle-1.3.2 shortuuid-1.0.11 smmap-5.0.0 spacy-3.3.2 tensorboardX-2.5.1 termcolor-1.1.0 thinc-8.0.17 tokenizers-0.12.1 torch-1.12.1 torchvision-0.13.1 transformers-4.20.1 typer-0.4.2 urllib3-1.26.14 wandb-0.12.21\n"]}]},{"cell_type":"code","source":["!pip install allennlp_models"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qym3PtmJte97","executionInfo":{"status":"ok","timestamp":1674898199902,"user_tz":-330,"elapsed":9428,"user":{"displayName":"Chitresh Kulhade","userId":"02463847632592786086"}},"outputId":"6b54f089-ea87-4126-c434-04ab0d896369"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting allennlp_models\n","  Downloading allennlp_models-2.10.1-py3-none-any.whl (464 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m464.5/464.5 KB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting word2number>=1.1\n","  Downloading word2number-1.1.zip (9.7 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting py-rouge==1.1\n","  Downloading py_rouge-1.1-py3-none-any.whl (56 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 KB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting conllu==4.4.2\n","  Downloading conllu-4.4.2-py2.py3-none-any.whl (15 kB)\n","Requirement already satisfied: torch<1.13.0,>=1.7.0 in /usr/local/lib/python3.8/dist-packages (from allennlp_models) (1.12.1)\n","Collecting ftfy\n","  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 KB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: allennlp<2.11,>=2.10.1 in /usr/local/lib/python3.8/dist-packages (from allennlp_models) (2.10.1)\n","Collecting datasets\n","  Downloading datasets-2.9.0-py3-none-any.whl (462 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.8/462.8 KB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: nltk>=3.6.5 in /usr/local/lib/python3.8/dist-packages (from allennlp_models) (3.7)\n","Requirement already satisfied: fairscale==0.4.6 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (0.4.6)\n","Requirement already satisfied: jsonnet>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (0.19.1)\n","Requirement already satisfied: base58>=2.1.1 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (2.1.1)\n","Requirement already satisfied: typer>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (0.4.2)\n","Requirement already satisfied: traitlets>5.1.1 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (5.7.1)\n","Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (2.28.2)\n","Requirement already satisfied: lmdb>=1.2.1 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (1.4.0)\n","Requirement already satisfied: dill>=0.3.4 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (0.3.6)\n","Requirement already satisfied: more-itertools>=8.12.0 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (9.0.0)\n","Requirement already satisfied: spacy<3.4,>=2.1.0 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (3.3.2)\n","Requirement already satisfied: transformers<4.21,>=4.1 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (4.20.1)\n","Requirement already satisfied: huggingface-hub>=0.0.16 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (0.10.1)\n","Requirement already satisfied: wandb<0.13.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (0.12.21)\n","Requirement already satisfied: numpy>=1.21.4 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (1.21.6)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (0.0.53)\n","Requirement already satisfied: sentencepiece>=0.1.96 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (0.1.97)\n","Requirement already satisfied: tensorboardX>=1.2 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (2.5.1)\n","Requirement already satisfied: h5py>=3.6.0 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (3.8.0)\n","Requirement already satisfied: cached-path<1.2.0,>=1.1.3 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (1.1.6)\n","Requirement already satisfied: scikit-learn>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (1.0.2)\n","Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (1.7.3)\n","Requirement already satisfied: tqdm>=4.62 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (4.64.1)\n","Requirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (1.1.0)\n","Requirement already satisfied: protobuf<4.0.0,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (3.19.6)\n","Requirement already satisfied: pytest>=6.2.5 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (7.2.1)\n","Requirement already satisfied: filelock<3.8,>=3.3 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (3.7.1)\n","Requirement already satisfied: torchvision<0.14.0,>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp_models) (0.13.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk>=3.6.5->allennlp_models) (1.2.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk>=3.6.5->allennlp_models) (2022.6.2)\n","Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk>=3.6.5->allennlp_models) (7.1.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch<1.13.0,>=1.7.0->allennlp_models) (4.4.0)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets->allennlp_models) (2022.11.0)\n","Collecting multiprocess\n","  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets->allennlp_models) (6.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets->allennlp_models) (21.3)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets->allennlp_models) (1.3.5)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets->allennlp_models) (3.8.3)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets->allennlp_models) (9.0.0)\n","Collecting xxhash\n","  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 KB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy->allennlp_models) (0.2.5)\n","Requirement already satisfied: rich<13.0,>=12.1 in /usr/local/lib/python3.8/dist-packages (from cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp_models) (12.6.0)\n","Requirement already satisfied: boto3<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp_models) (1.26.59)\n","Requirement already satisfied: google-cloud-storage<3.0,>=1.32.0 in /usr/local/lib/python3.8/dist-packages (from cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp_models) (2.7.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->allennlp_models) (1.3.3)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->allennlp_models) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->allennlp_models) (1.8.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->allennlp_models) (22.2.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->allennlp_models) (4.0.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->allennlp_models) (1.3.1)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->allennlp_models) (2.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets->allennlp_models) (3.0.9)\n","Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.8/dist-packages (from pytest>=6.2.5->allennlp<2.11,>=2.10.1->allennlp_models) (1.1.0)\n","Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.8/dist-packages (from pytest>=6.2.5->allennlp<2.11,>=2.10.1->allennlp_models) (1.0.0)\n","Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from pytest>=6.2.5->allennlp<2.11,>=2.10.1->allennlp_models) (2.0.1)\n","Requirement already satisfied: iniconfig in /usr/local/lib/python3.8/dist-packages (from pytest>=6.2.5->allennlp<2.11,>=2.10.1->allennlp_models) (2.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.28->allennlp<2.11,>=2.10.1->allennlp_models) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.28->allennlp<2.11,>=2.10.1->allennlp_models) (1.26.14)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.28->allennlp<2.11,>=2.10.1->allennlp_models) (2.10)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=1.0.1->allennlp<2.11,>=2.10.1->allennlp_models) (3.1.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp_models) (2.11.3)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp_models) (0.10.1)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp_models) (0.10.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp_models) (57.4.0)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp_models) (1.0.4)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp_models) (6.3.0)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp_models) (3.0.11)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp_models) (2.0.7)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp_models) (2.0.8)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp_models) (1.8.2)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp_models) (1.0.9)\n","Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp_models) (8.0.17)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp_models) (0.7.9)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp_models) (3.3.0)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp_models) (2.4.5)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp_models) (3.0.8)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision<0.14.0,>=0.8.1->allennlp<2.11,>=2.10.1->allennlp_models) (7.1.2)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers<4.21,>=4.1->allennlp<2.11,>=2.10.1->allennlp_models) (0.12.1)\n","Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.11,>=2.10.1->allennlp_models) (0.4.0)\n","Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.11,>=2.10.1->allennlp_models) (1.0.11)\n","Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.11,>=2.10.1->allennlp_models) (3.1.30)\n","Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.8/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.11,>=2.10.1->allennlp_models) (1.15.0)\n","Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.8/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.11,>=2.10.1->allennlp_models) (2.3)\n","Requirement already satisfied: pathtools in /usr/local/lib/python3.8/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.11,>=2.10.1->allennlp_models) (0.1.2)\n","Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.11,>=2.10.1->allennlp_models) (1.14.0)\n","Requirement already satisfied: setproctitle in /usr/local/lib/python3.8/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.11,>=2.10.1->allennlp_models) (1.3.2)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.11,>=2.10.1->allennlp_models) (5.4.8)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->allennlp_models) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->allennlp_models) (2022.7)\n","Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp_models) (1.0.1)\n","Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp_models) (0.6.0)\n","Requirement already satisfied: botocore<1.30.0,>=1.29.59 in /usr/local/lib/python3.8/dist-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp_models) (1.29.59)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp<2.11,>=2.10.1->allennlp_models) (4.0.10)\n","Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.8/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp_models) (2.11.0)\n","Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp_models) (2.3.2)\n","Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp_models) (2.4.0)\n","Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp_models) (2.16.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.8/dist-packages (from rich<13.0,>=12.1->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp_models) (2.6.1)\n","Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from rich<13.0,>=12.1->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp_models) (0.9.1)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp_models) (2.0.1)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp<2.11,>=2.10.1->allennlp_models) (5.0.0)\n","Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.8/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp_models) (1.58.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp_models) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp_models) (4.9)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp_models) (5.2.1)\n","Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.8/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp_models) (1.5.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp_models) (0.4.8)\n","Building wheels for collected packages: word2number\n","  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5582 sha256=6ed1bc746430353ceb352837a51c87f2395de81b9fae0ddefd8872e830f80215\n","  Stored in directory: /root/.cache/pip/wheels/cb/f3/5a/d88198fdeb46781ddd7e7f2653061af83e7adb2a076d8886d6\n","Successfully built word2number\n","Installing collected packages: word2number, py-rouge, xxhash, multiprocess, ftfy, conllu, responses, datasets, allennlp_models\n","Successfully installed allennlp_models-2.10.1 conllu-4.4.2 datasets-2.9.0 ftfy-6.1.1 multiprocess-0.70.14 py-rouge-1.1 responses-0.18.0 word2number-1.1 xxhash-3.2.0\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from tqdm import tqdm"],"metadata":{"id":"ewLHNB6OqrUy","executionInfo":{"status":"ok","timestamp":1674898145178,"user_tz":-330,"elapsed":1251,"user":{"displayName":"Chitresh Kulhade","userId":"02463847632592786086"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["import logging\n","from typing import Any, Dict, List, Optional\n","\n","import torch\n","from torch.nn.functional import nll_loss\n","\n","from allennlp.common.checks import check_dimensions_match\n","from allennlp.data import Vocabulary\n","from allennlp.models.model import Model\n","from allennlp.modules import Highway\n","from allennlp.modules import Seq2SeqEncoder, TimeDistributed, TextFieldEmbedder\n","from allennlp.modules.matrix_attention import MatrixAttention\n","from allennlp.nn import util, InitializerApplicator, RegularizerApplicator\n","from allennlp.training.metrics import BooleanAccuracy, CategoricalAccuracy\n","\n","from allennlp_models.rc.metrics import SquadEmAndF1\n","from allennlp_models.rc.models.utils import (\n","    get_best_span,\n","    replace_masked_values_with_big_negative_number,\n",")\n","\n","logger = logging.getLogger(__name__)"],"metadata":{"id":"r1LgGSLBqrW5","executionInfo":{"status":"ok","timestamp":1674898290260,"user_tz":-330,"elapsed":378,"user":{"displayName":"Chitresh Kulhade","userId":"02463847632592786086"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1OgQ155yt9QD","executionInfo":{"status":"ok","timestamp":1674898330031,"user_tz":-330,"elapsed":18647,"user":{"displayName":"Chitresh Kulhade","userId":"02463847632592786086"}},"outputId":"5c93b356-84fb-4900-8c9b-776cb0f2447f"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["df = pd.read_csv(\"/content/drive/MyDrive/NLP_hackathon/train_data.csv\")\n","df.drop(\"Unnamed: 0\",axis=1, inplace=True)\n","df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"P0in9t4kqrZF","executionInfo":{"status":"ok","timestamp":1674898372097,"user_tz":-330,"elapsed":1611,"user":{"displayName":"Chitresh Kulhade","userId":"02463847632592786086"}},"outputId":"3a287d0a-8397-4839-82ee-830ec8053e20"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     Theme                                          Paragraph  \\\n","0  Beyoncé  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n","1  Beyoncé  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n","2  Beyoncé  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n","3  Beyoncé  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n","4  Beyoncé  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n","\n","                                            Question  Answer_possible  \\\n","0  When did Beyonce leave Destiny's Child and bec...             True   \n","1      What album made her a worldwide known artist?             True   \n","2             Who managed the Destiny's Child group?             True   \n","3                     When did Beyoncé rise to fame?             True   \n","4     What role did Beyoncé have in Destiny's Child?             True   \n","\n","               Answer_text Answer_start  \n","0                 ['2003']        [526]  \n","1  ['Dangerously in Love']        [505]  \n","2       ['Mathew Knowles']        [360]  \n","3           ['late 1990s']        [276]  \n","4          ['lead singer']        [290]  "],"text/html":["\n","  <div id=\"df-800e581a-694c-4ae6-a2e5-47e5e3742f10\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Theme</th>\n","      <th>Paragraph</th>\n","      <th>Question</th>\n","      <th>Answer_possible</th>\n","      <th>Answer_text</th>\n","      <th>Answer_start</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Beyoncé</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","      <td>When did Beyonce leave Destiny's Child and bec...</td>\n","      <td>True</td>\n","      <td>['2003']</td>\n","      <td>[526]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Beyoncé</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","      <td>What album made her a worldwide known artist?</td>\n","      <td>True</td>\n","      <td>['Dangerously in Love']</td>\n","      <td>[505]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Beyoncé</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","      <td>Who managed the Destiny's Child group?</td>\n","      <td>True</td>\n","      <td>['Mathew Knowles']</td>\n","      <td>[360]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Beyoncé</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","      <td>When did Beyoncé rise to fame?</td>\n","      <td>True</td>\n","      <td>['late 1990s']</td>\n","      <td>[276]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Beyoncé</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","      <td>What role did Beyoncé have in Destiny's Child?</td>\n","      <td>True</td>\n","      <td>['lead singer']</td>\n","      <td>[290]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-800e581a-694c-4ae6-a2e5-47e5e3742f10')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-800e581a-694c-4ae6-a2e5-47e5e3742f10 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-800e581a-694c-4ae6-a2e5-47e5e3742f10');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["# preprocessing in text\n","df['Answer_text'] = df['Answer_text'].apply(lambda x: x.lstrip(\"[\").rstrip(\"]\").strip(\"'\").strip('''\"'''))\n","df['Answer_text'] = df['Answer_text'].apply(lambda x: x.replace(\"\\\\\",\"\"))\n","df['Answer_start'] = df['Answer_start'].apply(lambda x: x.lstrip('[').rstrip(']'))\n","df.iloc[37668, 4] = df.iloc[37668, 4].replace(\"ufeff\", \"\")\n","df.iloc[37668, 1] = df.iloc[37668, 1].replace(\"\\ufeff\", \"\")\n","df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"n8oVguBSqrbS","executionInfo":{"status":"ok","timestamp":1674898385452,"user_tz":-330,"elapsed":684,"user":{"displayName":"Chitresh Kulhade","userId":"02463847632592786086"}},"outputId":"fcfb6a34-caeb-4465-d244-2b2dc0cb69fe"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     Theme                                          Paragraph  \\\n","0  Beyoncé  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n","1  Beyoncé  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n","2  Beyoncé  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n","3  Beyoncé  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n","4  Beyoncé  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n","\n","                                            Question  Answer_possible  \\\n","0  When did Beyonce leave Destiny's Child and bec...             True   \n","1      What album made her a worldwide known artist?             True   \n","2             Who managed the Destiny's Child group?             True   \n","3                     When did Beyoncé rise to fame?             True   \n","4     What role did Beyoncé have in Destiny's Child?             True   \n","\n","           Answer_text Answer_start  \n","0                 2003          526  \n","1  Dangerously in Love          505  \n","2       Mathew Knowles          360  \n","3           late 1990s          276  \n","4          lead singer          290  "],"text/html":["\n","  <div id=\"df-c6ee1b97-6478-4a21-803e-e713e6bf7b7d\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Theme</th>\n","      <th>Paragraph</th>\n","      <th>Question</th>\n","      <th>Answer_possible</th>\n","      <th>Answer_text</th>\n","      <th>Answer_start</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Beyoncé</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","      <td>When did Beyonce leave Destiny's Child and bec...</td>\n","      <td>True</td>\n","      <td>2003</td>\n","      <td>526</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Beyoncé</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","      <td>What album made her a worldwide known artist?</td>\n","      <td>True</td>\n","      <td>Dangerously in Love</td>\n","      <td>505</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Beyoncé</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","      <td>Who managed the Destiny's Child group?</td>\n","      <td>True</td>\n","      <td>Mathew Knowles</td>\n","      <td>360</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Beyoncé</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","      <td>When did Beyoncé rise to fame?</td>\n","      <td>True</td>\n","      <td>late 1990s</td>\n","      <td>276</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Beyoncé</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","      <td>What role did Beyoncé have in Destiny's Child?</td>\n","      <td>True</td>\n","      <td>lead singer</td>\n","      <td>290</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c6ee1b97-6478-4a21-803e-e713e6bf7b7d')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c6ee1b97-6478-4a21-803e-e713e6bf7b7d button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c6ee1b97-6478-4a21-803e-e713e6bf7b7d');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["# import logging\n","# from typing import Any, Dict, List, Optional\n","\n","# import torch\n","# from torch.nn.functional import nll_loss\n","\n","# from allennlp.common.checks import check_dimensions_match\n","# from allennlp.data import Vocabulary\n","# from allennlp.models.model import Model\n","# from allennlp.modules import Highway\n","# from allennlp.modules import Seq2SeqEncoder, TimeDistributed, TextFieldEmbedder\n","# from allennlp.modules.matrix_attention import MatrixAttention\n","# from allennlp.nn import util, InitializerApplicator, RegularizerApplicator\n","# from allennlp.training.metrics import BooleanAccuracy, CategoricalAccuracy\n","\n","# from allennlp_models.rc.metrics import SquadEmAndF1\n","# from allennlp_models.rc.models.utils import (\n","#     get_best_span,\n","#     replace_masked_values_with_big_negative_number,\n","# )\n","\n","# logger = logging.getLogger(__name__)\n","\n","\n","class BidirectionalAttentionFlow(Model):\n","    \"\"\"\n","    This class implements Minjoon Seo's `Bidirectional Attention Flow model\n","    <https://www.semanticscholar.org/paper/Bidirectional-Attention-Flow-for-Machine-Seo-Kembhavi/7586b7cca1deba124af80609327395e613a20e9d>`_\n","    for answering reading comprehension questions (ICLR 2017).\n","    The basic layout is pretty simple: encode words as a combination of word embeddings and a\n","    character-level encoder, pass the word representations through a bi-LSTM/GRU, use a matrix of\n","    attentions to put question information into the passage word representations (this is the only\n","    part that is at all non-standard), pass this through another few layers of bi-LSTMs/GRUs, and\n","    do a softmax over span start and span end.\n","    # Parameters\n","    vocab : `Vocabulary`\n","    text_field_embedder : `TextFieldEmbedder`\n","        Used to embed the ``question`` and ``passage`` ``TextFields`` we get as input to the model.\n","    num_highway_layers : `int`\n","        The number of highway layers to use in between embedding the input and passing it through\n","        the phrase layer.\n","    phrase_layer : `Seq2SeqEncoder`\n","        The encoder (with its own internal stacking) that we will use in between embedding tokens\n","        and doing the bidirectional attention.\n","    matrix_attention : `MatrixAttention`\n","        The attention function that we will use when comparing encoded passage and question\n","        representations.\n","    modeling_layer : `Seq2SeqEncoder`\n","        The encoder (with its own internal stacking) that we will use in between the bidirectional\n","        attention and predicting span start and end.\n","    span_end_encoder : `Seq2SeqEncoder`\n","        The encoder that we will use to incorporate span start predictions into the passage state\n","        before predicting span end.\n","    dropout : `float`, optional (default=`0.2`)\n","        If greater than 0, we will apply dropout with this probability after all encoders (pytorch\n","        LSTMs do not apply dropout to their last layer).\n","    mask_lstms : `bool`, optional (default=`True`)\n","        If ``False``, we will skip passing the mask to the LSTM layers.  This gives a ~2x speedup,\n","        with only a slight performance decrease, if any.  We haven't experimented much with this\n","        yet, but have confirmed that we still get very similar performance with much faster\n","        training times.  We still use the mask for all softmaxes, but avoid the shuffling that's\n","        required when using masking with pytorch LSTMs.\n","    initializer : `InitializerApplicator`, optional (default=`InitializerApplicator()`)\n","        Used to initialize the model parameters.\n","    regularizer : `RegularizerApplicator`, optional (default=`None`)\n","        If provided, will be used to calculate the regularization penalty during training.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        vocab: Vocabulary,\n","        text_field_embedder: TextFieldEmbedder,\n","        num_highway_layers: int,\n","        phrase_layer: Seq2SeqEncoder,\n","        matrix_attention: MatrixAttention,\n","        modeling_layer: Seq2SeqEncoder,\n","        span_end_encoder: Seq2SeqEncoder,\n","        dropout: float = 0.2,\n","        mask_lstms: bool = True,\n","        initializer: InitializerApplicator = InitializerApplicator(),\n","        regularizer: Optional[RegularizerApplicator] = None,\n","    ) -> None:\n","        super().__init__(vocab, regularizer)\n","\n","        self._text_field_embedder = text_field_embedder\n","        self._highway_layer = TimeDistributed(\n","            Highway(text_field_embedder.get_output_dim(), num_highway_layers)\n","        )\n","        self._phrase_layer = phrase_layer\n","        self._matrix_attention = matrix_attention\n","        self._modeling_layer = modeling_layer\n","        self._span_end_encoder = span_end_encoder\n","\n","        encoding_dim = phrase_layer.get_output_dim()\n","        modeling_dim = modeling_layer.get_output_dim()\n","        span_start_input_dim = encoding_dim * 4 + modeling_dim\n","        self._span_start_predictor = TimeDistributed(torch.nn.Linear(span_start_input_dim, 1))\n","\n","        span_end_encoding_dim = span_end_encoder.get_output_dim()\n","        span_end_input_dim = encoding_dim * 4 + span_end_encoding_dim\n","        self._span_end_predictor = TimeDistributed(torch.nn.Linear(span_end_input_dim, 1))\n","\n","        # Bidaf has lots of layer dimensions which need to match up - these aren't necessarily\n","        # obvious from the configuration files, so we check here.\n","        check_dimensions_match(\n","            modeling_layer.get_input_dim(),\n","            4 * encoding_dim,\n","            \"modeling layer input dim\",\n","            \"4 * encoding dim\",\n","        )\n","        check_dimensions_match(\n","            text_field_embedder.get_output_dim(),\n","            phrase_layer.get_input_dim(),\n","            \"text field embedder output dim\",\n","            \"phrase layer input dim\",\n","        )\n","        check_dimensions_match(\n","            span_end_encoder.get_input_dim(),\n","            4 * encoding_dim + 3 * modeling_dim,\n","            \"span end encoder input dim\",\n","            \"4 * encoding dim + 3 * modeling dim\",\n","        )\n","\n","        self._span_start_accuracy = CategoricalAccuracy()\n","        self._span_end_accuracy = CategoricalAccuracy()\n","        self._span_accuracy = BooleanAccuracy()\n","        self._squad_metrics = SquadEmAndF1()\n","        if dropout > 0:\n","            self._dropout = torch.nn.Dropout(p=dropout)\n","        else:\n","            self._dropout = lambda x: x\n","        self._mask_lstms = mask_lstms\n","\n","        initializer(self)\n","\n","    def forward(  # type: ignore\n","        self,\n","        question: Dict[str, torch.LongTensor],\n","        passage: Dict[str, torch.LongTensor],\n","        span_start: torch.IntTensor = None,\n","        span_end: torch.IntTensor = None,\n","        metadata: List[Dict[str, Any]] = None,\n","    ) -> Dict[str, torch.Tensor]:\n","\n","        \"\"\"\n","        # Parameters\n","        question : `Dict[str, torch.LongTensor]`\n","            From a ``TextField``.\n","        passage : `Dict[str, torch.LongTensor]`\n","            From a ``TextField``.  The model assumes that this passage contains the answer to the\n","            question, and predicts the beginning and ending positions of the answer within the\n","            passage.\n","        span_start : `torch.IntTensor`, optional\n","            From an ``IndexField``.  This is one of the things we are trying to predict - the\n","            beginning position of the answer with the passage.  This is an `inclusive` token index.\n","            If this is given, we will compute a loss that gets included in the output dictionary.\n","        span_end : `torch.IntTensor`, optional\n","            From an ``IndexField``.  This is one of the things we are trying to predict - the\n","            ending position of the answer with the passage.  This is an `inclusive` token index.\n","            If this is given, we will compute a loss that gets included in the output dictionary.\n","        metadata : `List[Dict[str, Any]]`, optional\n","            metadata : `List[Dict[str, Any]]`, optional\n","            If present, this should contain the question tokens, passage tokens, original passage\n","            text, and token offsets into the passage for each instance in the batch.  The length\n","            of this list should be the batch size, and each dictionary should have the keys\n","            ``question_tokens``, ``passage_tokens``, ``original_passage``, and ``token_offsets``.\n","        Returns\n","        -------\n","        An output dictionary consisting of:\n","        span_start_logits : `torch.FloatTensor`\n","            A tensor of shape ``(batch_size, passage_length)`` representing unnormalized log\n","            probabilities of the span start position.\n","        span_start_probs : `torch.FloatTensor`\n","            The result of ``softmax(span_start_logits)``.\n","        span_end_logits : `torch.FloatTensor`\n","            A tensor of shape ``(batch_size, passage_length)`` representing unnormalized log\n","            probabilities of the span end position (inclusive).\n","        span_end_probs : `torch.FloatTensor`\n","            The result of ``softmax(span_end_logits)``.\n","        best_span : `torch.IntTensor`\n","            The result of a constrained inference over ``span_start_logits`` and\n","            ``span_end_logits`` to find the most probable span.  Shape is ``(batch_size, 2)``\n","            and each offset is a token index.\n","        loss : `torch.FloatTensor`, optional\n","            A scalar loss to be optimised.\n","        best_span_str : `List[str]`\n","            If sufficient metadata was provided for the instances in the batch, we also return the\n","            string from the original passage that the model thinks is the best answer to the\n","            question.\n","        \"\"\"\n","        embedded_question = self._highway_layer(self._text_field_embedder(question))\n","        embedded_passage = self._highway_layer(self._text_field_embedder(passage))\n","        batch_size = embedded_question.size(0)\n","        passage_length = embedded_passage.size(1)\n","        question_mask = util.get_text_field_mask(question)\n","        passage_mask = util.get_text_field_mask(passage)\n","        question_lstm_mask = question_mask if self._mask_lstms else None\n","        passage_lstm_mask = passage_mask if self._mask_lstms else None\n","\n","        encoded_question = self._dropout(self._phrase_layer(embedded_question, question_lstm_mask))\n","        encoded_passage = self._dropout(self._phrase_layer(embedded_passage, passage_lstm_mask))\n","        encoding_dim = encoded_question.size(-1)\n","\n","        # Shape: (batch_size, passage_length, question_length)\n","        passage_question_similarity = self._matrix_attention(encoded_passage, encoded_question)\n","        # Shape: (batch_size, passage_length, question_length)\n","        passage_question_attention = util.masked_softmax(passage_question_similarity, question_mask)\n","        # Shape: (batch_size, passage_length, encoding_dim)\n","        passage_question_vectors = util.weighted_sum(encoded_question, passage_question_attention)\n","\n","        # We replace masked values with something really negative here, so they don't affect the\n","        # max below.\n","        masked_similarity = replace_masked_values_with_big_negative_number(\n","            passage_question_similarity, question_mask.unsqueeze(1)\n","        )\n","        # Shape: (batch_size, passage_length)\n","        question_passage_similarity = masked_similarity.max(dim=-1)[0].squeeze(-1)\n","        # Shape: (batch_size, passage_length)\n","        question_passage_attention = util.masked_softmax(question_passage_similarity, passage_mask)\n","        # Shape: (batch_size, encoding_dim)\n","        question_passage_vector = util.weighted_sum(encoded_passage, question_passage_attention)\n","        # Shape: (batch_size, passage_length, encoding_dim)\n","        tiled_question_passage_vector = question_passage_vector.unsqueeze(1).expand(\n","            batch_size, passage_length, encoding_dim\n","        )\n","\n","        # Shape: (batch_size, passage_length, encoding_dim * 4)\n","        final_merged_passage = torch.cat(\n","            [\n","                encoded_passage,\n","                passage_question_vectors,\n","                encoded_passage * passage_question_vectors,\n","                encoded_passage * tiled_question_passage_vector,\n","            ],\n","            dim=-1,\n","        )\n","\n","        modeled_passage = self._dropout(\n","            self._modeling_layer(final_merged_passage, passage_lstm_mask)\n","        )\n","        modeling_dim = modeled_passage.size(-1)\n","\n","        # Shape: (batch_size, passage_length, encoding_dim * 4 + modeling_dim))\n","        span_start_input = self._dropout(torch.cat([final_merged_passage, modeled_passage], dim=-1))\n","        # Shape: (batch_size, passage_length)\n","        span_start_logits = self._span_start_predictor(span_start_input).squeeze(-1)\n","        # Shape: (batch_size, passage_length)\n","        span_start_probs = util.masked_softmax(span_start_logits, passage_mask)\n","\n","        # Shape: (batch_size, modeling_dim)\n","        span_start_representation = util.weighted_sum(modeled_passage, span_start_probs)\n","        # Shape: (batch_size, passage_length, modeling_dim)\n","        tiled_start_representation = span_start_representation.unsqueeze(1).expand(\n","            batch_size, passage_length, modeling_dim\n","        )\n","\n","        # Shape: (batch_size, passage_length, encoding_dim * 4 + modeling_dim * 3)\n","        span_end_representation = torch.cat(\n","            [\n","                final_merged_passage,\n","                modeled_passage,\n","                tiled_start_representation,\n","                modeled_passage * tiled_start_representation,\n","            ],\n","            dim=-1,\n","        )\n","        # Shape: (batch_size, passage_length, encoding_dim)\n","        encoded_span_end = self._dropout(\n","            self._span_end_encoder(span_end_representation, passage_lstm_mask)\n","        )\n","        # Shape: (batch_size, passage_length, encoding_dim * 4 + span_end_encoding_dim)\n","        span_end_input = self._dropout(torch.cat([final_merged_passage, encoded_span_end], dim=-1))\n","        span_end_logits = self._span_end_predictor(span_end_input).squeeze(-1)\n","        span_end_probs = util.masked_softmax(span_end_logits, passage_mask)\n","\n","        # Replace the masked values with a very negative constant.\n","        span_start_logits = replace_masked_values_with_big_negative_number(\n","            span_start_logits, passage_mask\n","        )\n","        span_end_logits = replace_masked_values_with_big_negative_number(\n","            span_end_logits, passage_mask\n","        )\n","        best_span = get_best_span(span_start_logits, span_end_logits)\n","\n","        output_dict = {\n","            \"passage_question_attention\": passage_question_attention,\n","            \"span_start_logits\": span_start_logits,\n","            \"span_start_probs\": span_start_probs,\n","            \"span_end_logits\": span_end_logits,\n","            \"span_end_probs\": span_end_probs,\n","            \"best_span\": best_span,\n","        }\n","\n","        # Compute the loss for training.\n","        if span_start is not None:\n","            loss = nll_loss(\n","                util.masked_log_softmax(span_start_logits, passage_mask), span_start.squeeze(-1)\n","            )\n","            self._span_start_accuracy(span_start_logits, span_start.squeeze(-1))\n","            loss += nll_loss(\n","                util.masked_log_softmax(span_end_logits, passage_mask), span_end.squeeze(-1)\n","            )\n","            self._span_end_accuracy(span_end_logits, span_end.squeeze(-1))\n","            self._span_accuracy(best_span, torch.cat([span_start, span_end], -1))\n","            output_dict[\"loss\"] = loss\n","\n","        # Compute the EM and F1 on SQuAD and add the tokenized input to the output.\n","        if metadata is not None:\n","            output_dict[\"best_span_str\"] = []\n","            question_tokens = []\n","            passage_tokens = []\n","            token_offsets = []\n","            for i in range(batch_size):\n","                question_tokens.append(metadata[i][\"question_tokens\"])\n","                passage_tokens.append(metadata[i][\"passage_tokens\"])\n","                token_offsets.append(metadata[i][\"token_offsets\"])\n","                passage_str = metadata[i][\"original_passage\"]\n","                offsets = metadata[i][\"token_offsets\"]\n","                predicted_span = tuple(best_span[i].detach().cpu().numpy())\n","                start_offset = offsets[predicted_span[0]][0]\n","                end_offset = offsets[predicted_span[1]][1]\n","                best_span_string = passage_str[start_offset:end_offset]\n","                output_dict[\"best_span_str\"].append(best_span_string)\n","                answer_texts = metadata[i].get(\"answer_texts\", [])\n","                if answer_texts:\n","                    self._squad_metrics(best_span_string, answer_texts)\n","            output_dict[\"question_tokens\"] = question_tokens\n","            output_dict[\"passage_tokens\"] = passage_tokens\n","            output_dict[\"token_offsets\"] = token_offsets\n","        return output_dict\n","\n","    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n","        exact_match, f1_score = self._squad_metrics.get_metric(reset)\n","        return {\n","            \"start_acc\": self._span_start_accuracy.get_metric(reset),\n","            \"end_acc\": self._span_end_accuracy.get_metric(reset),\n","            \"span_acc\": self._span_accuracy.get_metric(reset),\n","            \"em\": exact_match,\n","            \"f1\": f1_score,\n","        }\n","\n","    @staticmethod\n","    def get_best_span(\n","        span_start_logits: torch.Tensor, span_end_logits: torch.Tensor\n","    ) -> torch.Tensor:\n","        # We call the inputs \"logits\" - they could either be unnormalized logits or normalized log\n","        # probabilities.  A log_softmax operation is a constant shifting of the entire logit\n","        # vector, so taking an argmax over either one gives the same result.\n","        if span_start_logits.dim() != 2 or span_end_logits.dim() != 2:\n","            raise ValueError(\"Input shapes must be (batch_size, passage_length)\")\n","        batch_size, passage_length = span_start_logits.size()\n","        device = span_start_logits.device\n","        # (batch_size, passage_length, passage_length)\n","        span_log_probs = span_start_logits.unsqueeze(2) + span_end_logits.unsqueeze(1)\n","        # Only the upper triangle of the span matrix is valid; the lower triangle has entries where\n","        # the span ends before it starts.\n","        span_log_mask = (\n","            torch.triu(torch.ones((passage_length, passage_length), device=device))\n","            .log()\n","            .unsqueeze(0)\n","        )\n","        valid_span_log_probs = span_log_probs + span_log_mask\n","\n","        # Here we take the span matrix and flatten it, then find the best span using argmax.  We\n","        # can recover the start and end indices from this flattened list using simple modular\n","        # arithmetic.\n","        # (batch_size, passage_length * passage_length)\n","        best_spans = valid_span_log_probs.view(batch_size, -1).argmax(-1)\n","        span_start_indices = best_spans // passage_length\n","        span_end_indices = best_spans % passage_length\n","        return torch.stack([span_start_indices, span_end_indices], dim=-1)\n","\n","    default_predictor = \"reading_comprehension\""],"metadata":{"id":"9RFM-qPeqrdd","executionInfo":{"status":"ok","timestamp":1674898425632,"user_tz":-330,"elapsed":396,"user":{"displayName":"Chitresh Kulhade","userId":"02463847632592786086"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["!pip install bidaf-keras"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EZ3K6nC1qrf3","executionInfo":{"status":"ok","timestamp":1674902494111,"user_tz":-330,"elapsed":2254,"user":{"displayName":"Chitresh Kulhade","userId":"02463847632592786086"}},"outputId":"3a7ea16d-d1e5-460b-afe2-380f3840ae1c"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: bidaf-keras in /usr/local/lib/python3.8/dist-packages (1.0.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from bidaf-keras) (3.7)\n","Requirement already satisfied: keras in /usr/local/lib/python3.8/dist-packages (from bidaf-keras) (2.9.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from bidaf-keras) (4.64.1)\n","Requirement already satisfied: pymagnitude in /usr/local/lib/python3.8/dist-packages (from bidaf-keras) (0.1.143)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk->bidaf-keras) (2022.6.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->bidaf-keras) (1.2.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->bidaf-keras) (7.1.2)\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3021, in _dep_map\n","    return self.__dep_map\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2815, in __getattr__\n","    raise AttributeError(attr)\n","AttributeError: _DistInfoDistribution__dep_map\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/base_command.py\", line 167, in exc_logging_wrapper\n","    status = run_func(*args)\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/req_command.py\", line 199, in wrapper\n","    return func(self, options, args)\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/commands/install.py\", line 397, in run\n","    conflicts = self._determine_conflicts(to_install)\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/commands/install.py\", line 529, in _determine_conflicts\n","    return check_install_conflicts(to_install)\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n","    package_set, _ = create_package_set_from_installed()\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/operations/check.py\", line 42, in create_package_set_from_installed\n","    dependencies = list(dist.iter_dependencies())\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/metadata/pkg_resources.py\", line 202, in iter_dependencies\n","    return self._dist.requires(extras)\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2736, in requires\n","    dm = self._dep_map\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3023, in _dep_map\n","    self.__dep_map = self._compute_dependencies()\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3033, in _compute_dependencies\n","    reqs.extend(parse_requirements(req))\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3094, in parse_requirements\n","    yield Requirement(line)\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3101, in __init__\n","    super(Requirement, self).__init__(requirement_string)\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/packaging/requirements.py\", line 102, in __init__\n","    req = REQUIREMENT.parseString(requirement_string)\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 1124, in parse_string\n","    loc, tokens = self._parse(instring, 0)\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 810, in _parseNoCache\n","    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 3863, in parseImpl\n","    loc, exprtokens = e._parse(instring, loc, doActions)\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 810, in _parseNoCache\n","    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 4091, in parseImpl\n","    return e._parse(\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 810, in _parseNoCache\n","    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 3863, in parseImpl\n","    loc, exprtokens = e._parse(instring, loc, doActions)\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 810, in _parseNoCache\n","    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 4936, in parseImpl\n","    loc, tokens = self_expr._parse(instring, loc, doActions, callPreParse=False)\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 810, in _parseNoCache\n","    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 3863, in parseImpl\n","    loc, exprtokens = e._parse(instring, loc, doActions)\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 810, in _parseNoCache\n","    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 3863, in parseImpl\n","    loc, exprtokens = e._parse(instring, loc, doActions)\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 810, in _parseNoCache\n","    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 5203, in parseImpl\n","    return super().parseImpl(instring, loc, doActions)\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 4352, in parseImpl\n","    return self.expr._parse(instring, loc, doActions, callPreParse=False)\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 810, in _parseNoCache\n","    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 3841, in parseImpl\n","    loc, resultlist = self.exprs[0]._parse(\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 810, in _parseNoCache\n","    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 4091, in parseImpl\n","    return e._parse(\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 810, in _parseNoCache\n","    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 4352, in parseImpl\n","    return self.expr._parse(instring, loc, doActions, callPreParse=False)\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 810, in _parseNoCache\n","    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 3863, in parseImpl\n","    loc, exprtokens = e._parse(instring, loc, doActions)\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 810, in _parseNoCache\n","    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 4091, in parseImpl\n","    return e._parse(\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 810, in _parseNoCache\n","    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 4091, in parseImpl\n","    return e._parse(\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 818, in _parseNoCache\n","    ret_tokens = ParseResults(\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/results.py\", line 153, in __new__\n","    self._toklist = [toklist]\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/pip3\", line 8, in <module>\n","    sys.exit(main())\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/main.py\", line 70, in main\n","    return command.main(cmd_args)\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n","    return self._main(args)\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/base_command.py\", line 221, in _main\n","    return run(options, args)\n","  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/base_command.py\", line 204, in exc_logging_wrapper\n","    logger.critical(\"Operation cancelled by user\")\n","  File \"/usr/lib/python3.8/logging/__init__.py\", line 1493, in critical\n","    self._log(CRITICAL, msg, args, **kwargs)\n","  File \"/usr/lib/python3.8/logging/__init__.py\", line 1577, in _log\n","    fn, lno, func, sinfo = self.findCaller(stack_info, stacklevel)\n","  File \"/usr/lib/python3.8/logging/__init__.py\", line 1519, in findCaller\n","    f = currentframe()\n","  File \"/usr/lib/python3.8/logging/__init__.py\", line 160, in <lambda>\n","    currentframe = lambda: sys._getframe(3)\n","KeyboardInterrupt\n","^C\n"]}]},{"cell_type":"code","source":["!pip install keras.engine.topology"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9QKdpqBm9-7K","executionInfo":{"status":"ok","timestamp":1674902511562,"user_tz":-330,"elapsed":4036,"user":{"displayName":"Chitresh Kulhade","userId":"02463847632592786086"}},"outputId":"83612641-493b-4e09-ea20-8c95de45ff2d"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","\u001b[31mERROR: Could not find a version that satisfies the requirement keras.engine.topology (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for keras.engine.topology\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["from bidaf.models import BidirectionalAttentionFlow\n","from bidaf.scripts import load_data_generators\n","\n","bidaf_model = BidirectionalAttentionFlow(400)\n","bidaf_model.load_bidaf(\"/content/drive/MyDrive/NLP_hackathon/bidaf_50.h5\") # when you want to resume training\n","train_generator, validation_generator = load_data_generators(24, 400)\n","keras_model = bidaf_model.train_model(train_generator, validation_generator=validation_generator)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":844},"id":"GEf4wOkhqrkA","executionInfo":{"status":"error","timestamp":1674902471803,"user_tz":-330,"elapsed":441,"user":{"displayName":"Chitresh Kulhade","userId":"02463847632592786086"}},"outputId":"c462412c-dcfa-47ab-c0b4-2c9fed85eb4e"},"execution_count":17,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-36e80cdd8662>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbidaf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBidirectionalAttentionFlow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbidaf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscripts\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_data_generators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbidaf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBidirectionalAttentionFlow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbidaf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_bidaf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/NLP_hackathon/bidaf_50.h5\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# when you want to resume training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/bidaf/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbidaf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBidirectionalAttentionFlow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/bidaf/models/bidaf.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdadelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCSVLogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHighway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSimilarity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC2QAttention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ2CAttention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMergedContext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSpanBegin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSpanEnd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCombineOutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscripts\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnegative_avg_log_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMagnitudeVectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_best_span\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mget_word_char_loc_mapping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/bidaf/layers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mhighway_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHighway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msimilarity_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimilarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcontext_to_query\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mC2QAttention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mquery_to_context\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mQ2CAttention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmerged_context\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMergedContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/bidaf/layers/highway_layer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopology\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultiply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConstant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.engine.topology'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":[],"metadata":{"id":"MUxo-e6tqrmP"},"execution_count":null,"outputs":[]}]}