{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "EvTgY_OkdLkf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d935bb4-d462-48b1-d12e-de3affe20396"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6Ix2N9xP3Tq",
        "outputId": "dadda2c2-bbaf-42d8-e1f8-52139d4316ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m101.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1n4nrhTp5rw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "import pandas as pd\n",
        "import transformers\n",
        "from transformers import BertTokenizerFast\n",
        "from transformers import BertModel, BertConfig, BertPreTrainedModel\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/QA/train_data.csv\")\n",
        "df.drop(\"Unnamed: 0\",axis=1, inplace=True)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "dGV5nqML0TOB",
        "outputId": "e716e3f4-94c4-4a23-9008-e6fe746bab88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Theme                                          Paragraph  \\\n",
              "0  Beyoncé  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
              "1  Beyoncé  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
              "2  Beyoncé  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
              "3  Beyoncé  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
              "4  Beyoncé  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
              "\n",
              "                                            Question  Answer_possible  \\\n",
              "0  When did Beyonce leave Destiny's Child and bec...             True   \n",
              "1      What album made her a worldwide known artist?             True   \n",
              "2             Who managed the Destiny's Child group?             True   \n",
              "3                     When did Beyoncé rise to fame?             True   \n",
              "4     What role did Beyoncé have in Destiny's Child?             True   \n",
              "\n",
              "               Answer_text Answer_start  \n",
              "0                 ['2003']        [526]  \n",
              "1  ['Dangerously in Love']        [505]  \n",
              "2       ['Mathew Knowles']        [360]  \n",
              "3           ['late 1990s']        [276]  \n",
              "4          ['lead singer']        [290]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-064e7f6d-ea45-48a5-b760-e445582cae4a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Theme</th>\n",
              "      <th>Paragraph</th>\n",
              "      <th>Question</th>\n",
              "      <th>Answer_possible</th>\n",
              "      <th>Answer_text</th>\n",
              "      <th>Answer_start</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
              "      <td>True</td>\n",
              "      <td>['2003']</td>\n",
              "      <td>[526]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>What album made her a worldwide known artist?</td>\n",
              "      <td>True</td>\n",
              "      <td>['Dangerously in Love']</td>\n",
              "      <td>[505]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>Who managed the Destiny's Child group?</td>\n",
              "      <td>True</td>\n",
              "      <td>['Mathew Knowles']</td>\n",
              "      <td>[360]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>When did Beyoncé rise to fame?</td>\n",
              "      <td>True</td>\n",
              "      <td>['late 1990s']</td>\n",
              "      <td>[276]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>What role did Beyoncé have in Destiny's Child?</td>\n",
              "      <td>True</td>\n",
              "      <td>['lead singer']</td>\n",
              "      <td>[290]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-064e7f6d-ea45-48a5-b760-e445582cae4a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-064e7f6d-ea45-48a5-b760-e445582cae4a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-064e7f6d-ea45-48a5-b760-e445582cae4a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.Theme.unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64aysiZSIWE9",
        "outputId": "3e45995c-d871-4e27-fb94-6f8574369859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Beyoncé', 'Frédéric_Chopin',\n",
              "       'Sino-Tibetan_relations_during_the_Ming_dynasty',\n",
              "       'The_Legend_of_Zelda:_Twilight_Princess', 'Spectre_(2015_film)',\n",
              "       'New_York_City', 'To_Kill_a_Mockingbird', 'Solar_energy',\n",
              "       'Kanye_West', 'Buddhism', 'American_Idol', 'Dog',\n",
              "       '2008_Summer_Olympics_torch_relay', 'Genome',\n",
              "       'Comprehensive_school', 'Republic_of_the_Congo', 'Prime_minister',\n",
              "       'Institute_of_technology', 'Dutch_Republic', 'Symbiosis',\n",
              "       'Iranian_languages', 'Lighting',\n",
              "       'Separation_of_powers_under_the_United_States_Constitution',\n",
              "       'Architecture', 'Southern_Europe', 'BBC_Television',\n",
              "       'Arnold_Schwarzenegger', 'Plymouth', 'Christian',\n",
              "       'Sony_Music_Entertainment', 'Oklahoma_City', 'Hunter-gatherer',\n",
              "       'United_Nations_Population_Fund',\n",
              "       'Russian_Soviet_Federative_Socialist_Republic',\n",
              "       'Alexander_Graham_Bell', 'Internet_service_provider', 'Comics',\n",
              "       'Saint_Helena', 'Aspirated_consonant', 'Hydrogen', 'Space_Race',\n",
              "       'BeiDou_Navigation_Satellite_System', 'Canon_law',\n",
              "       'Communications_in_Somalia', 'Boston', 'Universal_Studios',\n",
              "       'Estonian_language', 'Daylight_saving_time',\n",
              "       'Royal_Institute_of_British_Architects',\n",
              "       'National_Archives_and_Records_Administration', 'Tristan_da_Cunha',\n",
              "       'University_of_Kansas', 'Arena_Football_League', 'Bern',\n",
              "       'Westminster_Abbey', 'Political_corruption', 'Classical_music',\n",
              "       'Slavs', 'Treaty', 'Josip_Broz_Tito', 'Marshall_Islands',\n",
              "       'Szlachta', 'Virgil', 'Alps', 'Gene', 'Guinea-Bissau',\n",
              "       'List_of_numbered_streets_in_Manhattan', 'Brain', 'Near_East',\n",
              "       'Zhejiang', 'Ministry_of_Defence_(United_Kingdom)',\n",
              "       'High-definition_television', 'Wood', 'Somalis', 'Middle_Ages',\n",
              "       'Phonology', 'Computer', 'Black_people', 'New_Delhi',\n",
              "       'Bird_migration', 'Atlantic_City,_New_Jersey', 'MP3',\n",
              "       'House_music', 'Letter_case', 'Chihuahua_(state)', 'Pitch_(music)',\n",
              "       'England_national_football_team', 'Houston', 'Copper',\n",
              "       'Identity_(social_science)', 'Himachal_Pradesh', 'Communication',\n",
              "       'Computer_security', 'Orthodox_Judaism', 'Animal', 'Beer',\n",
              "       'Race_and_ethnicity_in_the_United_States_Census',\n",
              "       'Imperial_College_London', 'Hanover', 'Emotion', 'Old_English',\n",
              "       'Aircraft_carrier', 'Federal_Aviation_Administration',\n",
              "       'Lancashire', 'Mesozoic', 'Videoconferencing',\n",
              "       'Gregorian_calendar', 'Xbox_360',\n",
              "       'Military_history_of_the_United_States', 'Infrared', 'ASCII',\n",
              "       'Digestion', 'Gymnastics', 'FC_Barcelona', 'Melbourne',\n",
              "       'John,_King_of_England', 'Macintosh', 'Valencia',\n",
              "       'General_Electric', 'United_States_Army', 'Franco-Prussian_War',\n",
              "       'Adolescence', 'Antarctica', 'Eritrea', 'Uranium',\n",
              "       'Circadian_rhythm', 'Sexual_orientation', 'Dell',\n",
              "       'Nintendo_Entertainment_System', 'Seattle', 'Memory',\n",
              "       'Multiracial_American', 'Ashkenazi_Jews',\n",
              "       'Pharmaceutical_industry', 'Umayyad_Caliphate', 'Asphalt',\n",
              "       'Queen_Victoria', 'Israel', 'Hellenistic_period',\n",
              "       'Bill_%26_Melinda_Gates_Foundation', 'Dutch_language',\n",
              "       'Buckingham_Palace', 'Incandescent_light_bulb', 'Arsenal_F.C.',\n",
              "       'Chicago_Cubs', 'Korean_War', 'Copyright_infringement', 'Greece',\n",
              "       'Royal_Dutch_Shell', 'Mammal', 'East_India_Company', 'Hokkien',\n",
              "       'Professional_wrestling', 'Film_speed', 'Mexico_City', 'Napoleon',\n",
              "       'Germans', 'Southeast_Asia', 'Brigham_Young_University',\n",
              "       'Intellectual_property', 'Florida', 'Queen_(band)',\n",
              "       'Presbyterianism', 'Thuringia', 'Predation', 'British_Empire',\n",
              "       'Botany', 'Madonna_(entertainer)', 'Law_of_the_United_States',\n",
              "       'Myanmar', 'Jews', 'Cotton', 'Data_compression',\n",
              "       'The_Sun_(United_Kingdom)', 'Pesticide', 'Somerset',\n",
              "       'Yale_University', 'Late_Middle_Ages', 'Ann_Arbor,_Michigan',\n",
              "       'Gothic_architecture', 'Cubism', 'Political_philosophy',\n",
              "       'Norfolk_Island', 'Edmund_Burke', 'Samoa', 'Pope_Paul_VI',\n",
              "       'Switzerland', 'Mali', 'Raleigh,_North_Carolina', 'Crimean_War',\n",
              "       'Nonprofit_organization', 'Literature', 'Avicenna', 'Nigeria',\n",
              "       'Molotov%E2%80%93Ribbentrop_Pact', 'History_of_science', 'Digimon',\n",
              "       'Glacier', 'Affirmative_action_in_the_United_States', 'FA_Cup',\n",
              "       'New_Haven,_Connecticut', 'Alsace', 'Carnival', 'Baptists',\n",
              "       'Child_labour', 'Dissolution_of_the_Soviet_Union',\n",
              "       'Crucifixion_of_Jesus', 'Supreme_court', 'Textual_criticism',\n",
              "       'Gramophone_record', 'Turner_Classic_Movies', 'Hindu_philosophy',\n",
              "       'A_cappella', 'Dominican_Order', 'Eton_College', 'Cork_(city)',\n",
              "       'Galicia_(Spain)', 'USB', 'Sichuan', 'Unicode', 'Detroit',\n",
              "       'London', 'Culture', 'Sahara', 'Rule_of_law', 'Exhibition_game',\n",
              "       'Northwestern_University', 'Strasbourg', 'History_of_India',\n",
              "       'Gamal_Abdel_Nasser', 'Pope_John_XXIII', 'Time',\n",
              "       'St._John%27s,_Newfoundland_and_Labrador', 'John_von_Neumann',\n",
              "       'PlayStation_3', 'Royal_assent', 'Central_African_Republic',\n",
              "       'Asthma', 'LaserDisc', 'George_VI', 'Federalism', 'Annelid',\n",
              "       'War_on_Terror', 'Labour_Party_(UK)', 'Estonia', 'Alaska',\n",
              "       'Mandolin', 'Insect', 'Race_(human_categorization)', 'Paris',\n",
              "       'Apollo', 'United_States_presidential_election,_2004',\n",
              "       'Liberal_Party_of_Australia', 'Samurai', 'Software_testing',\n",
              "       'States_of_Germany', 'Glass', 'Renewable_energy_commercialization',\n",
              "       'Palermo', 'Green', 'Neoclassical_architecture', 'Serbo-Croatian',\n",
              "       'CBC_Television', 'IBM', 'Energy', 'East_Prussia',\n",
              "       'Ottoman_Empire', 'Philosophy_of_space_and_time', 'Neolithic',\n",
              "       'Friedrich_Hayek', 'Diarrhea', 'Madrasa', 'Miami', 'Philadelphia',\n",
              "       'John_Kerry', 'Rajasthan', 'Guam', 'Empiricism', 'Idealism',\n",
              "       'Czech_language', 'Education', 'Tennessee', 'Canadian_football',\n",
              "       'Seven_Years%27_War', 'Cyprus', 'Steven_Spielberg', 'Elevator',\n",
              "       'Railway_electrification_system',\n",
              "       'Spanish_language_in_the_United_States',\n",
              "       'Charleston,_South_Carolina', 'The_Blitz', 'Vacuum', 'Quran',\n",
              "       'Geography_of_the_United_States', 'Compact_disc', 'Modern_history',\n",
              "       'Flowering_plant', 'Hyderabad', 'Santa_Monica,_California', 'Pain',\n",
              "       'Database', 'Tucson,_Arizona', 'Bacteria', 'Printed_circuit_board',\n",
              "       'Greeks', 'Premier_League', 'Roman_Republic', 'Pacific_War',\n",
              "       'San_Diego', 'Iran', 'British_Isles', 'Association_football',\n",
              "       'Georgian_architecture', 'Liberia', 'Alfred_North_Whitehead',\n",
              "       'Antibiotics', 'Windows_8', 'Swaziland', 'Translation', 'Airport',\n",
              "       'Super_Nintendo_Entertainment_System', 'Sumer', 'Tuvalu',\n",
              "       'Namibia', 'Russian_language', 'United_States_Air_Force',\n",
              "       'Light-emitting_diode', 'Great_power', 'Bird', 'Qing_dynasty',\n",
              "       'Indigenous_peoples_of_the_Americas', 'Red', 'Mosaic',\n",
              "       'University', 'Religion_in_ancient_Rome', 'YouTube',\n",
              "       'Separation_of_church_and_state_in_the_United_States',\n",
              "       'Bras%C3%ADlia', 'Economy_of_Greece',\n",
              "       'Party_leaders_of_the_United_States_House_of_Representatives',\n",
              "       'Armenians', 'Jehovah%27s_Witnesses', 'Dwight_D._Eisenhower',\n",
              "       'The_Bronx', 'Financial_crisis_of_2007%E2%80%9308', 'Portugal',\n",
              "       'Humanism', 'Geological_history_of_Earth', 'Police', 'Genocide',\n",
              "       'Saint_Barth%C3%A9lemy', 'Tajikistan', 'University_of_Notre_Dame',\n",
              "       'Anthropology', 'Montana', 'Punjab,_Pakistan', 'Infection',\n",
              "       'Hunting', 'Kathmandu', 'Myocardial_infarction', 'Matter'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing in text\n",
        "df['Answer_text'] = df['Answer_text'].apply(lambda x: x.lstrip(\"[\").rstrip(\"]\").strip(\"'\").strip('''\"'''))\n",
        "df['Answer_text'] = df['Answer_text'].apply(lambda x: x.replace(\"\\\\\",\"\"))\n",
        "df['Answer_start'] = df['Answer_start'].apply(lambda x: x.lstrip('[').rstrip(']'))\n",
        "df.iloc[37668, 4] = df.iloc[37668, 4].replace(\"ufeff\", \"\")\n",
        "df.iloc[37668, 1] = df.iloc[37668, 1].replace(\"\\ufeff\", \"\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "wpF0Malcadlg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "785db26e-419e-4645-b3ae-8cd4feca4ae4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Theme                                          Paragraph  \\\n",
              "0  Beyoncé  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
              "1  Beyoncé  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
              "2  Beyoncé  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
              "3  Beyoncé  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
              "4  Beyoncé  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
              "\n",
              "                                            Question  Answer_possible  \\\n",
              "0  When did Beyonce leave Destiny's Child and bec...             True   \n",
              "1      What album made her a worldwide known artist?             True   \n",
              "2             Who managed the Destiny's Child group?             True   \n",
              "3                     When did Beyoncé rise to fame?             True   \n",
              "4     What role did Beyoncé have in Destiny's Child?             True   \n",
              "\n",
              "           Answer_text Answer_start  \n",
              "0                 2003          526  \n",
              "1  Dangerously in Love          505  \n",
              "2       Mathew Knowles          360  \n",
              "3           late 1990s          276  \n",
              "4          lead singer          290  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0ac0cf5f-48b4-41ce-8fb7-e28121ca967f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Theme</th>\n",
              "      <th>Paragraph</th>\n",
              "      <th>Question</th>\n",
              "      <th>Answer_possible</th>\n",
              "      <th>Answer_text</th>\n",
              "      <th>Answer_start</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
              "      <td>True</td>\n",
              "      <td>2003</td>\n",
              "      <td>526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>What album made her a worldwide known artist?</td>\n",
              "      <td>True</td>\n",
              "      <td>Dangerously in Love</td>\n",
              "      <td>505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>Who managed the Destiny's Child group?</td>\n",
              "      <td>True</td>\n",
              "      <td>Mathew Knowles</td>\n",
              "      <td>360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>When did Beyoncé rise to fame?</td>\n",
              "      <td>True</td>\n",
              "      <td>late 1990s</td>\n",
              "      <td>276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>What role did Beyoncé have in Destiny's Child?</td>\n",
              "      <td>True</td>\n",
              "      <td>lead singer</td>\n",
              "      <td>290</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0ac0cf5f-48b4-41ce-8fb7-e28121ca967f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0ac0cf5f-48b4-41ce-8fb7-e28121ca967f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0ac0cf5f-48b4-41ce-8fb7-e28121ca967f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(df.loc[0, 'Answer_possible'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFAfcGw78-xy",
        "outputId": "045dbcf1-9397-46ef-810b-22f306eddba8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.bool_"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_end_idx(df):\n",
        "  end_idx_list = []\n",
        "  for i in range(len(df)):\n",
        "    gold_text = df.iloc[i, 4]\n",
        "    context = df.iloc[i,1]\n",
        "\n",
        "    if df.iloc[i,3] == True:\n",
        "      start_idx = int(df.iloc[i, 5])\n",
        "      end_idx = start_idx + len(gold_text)\n",
        "      \n",
        "\n",
        "      # sometimes squad answers are off by a character or two so we fix this\n",
        "      if context[start_idx : end_idx] == gold_text:\n",
        "        df.iloc[i, 5] = start_idx\n",
        "        end_idx_list.append(end_idx)\n",
        "      elif context[start_idx - 1:end_idx - 1] == gold_text:\n",
        "        df.iloc[i, 5] = start_idx - 1\n",
        "        end_idx_list.append(end_idx - 1) \n",
        "      elif context[start_idx + 1:end_idx + 1] == gold_text:\n",
        "        df.iloc[i, 5] = start_idx + 1\n",
        "        end_idx_list.append(end_idx + 1)     \n",
        "      elif context[start_idx - 2:end_idx - 2] == gold_text:\n",
        "        df.iloc[i, 5] = start_idx - 2\n",
        "        end_idx_list.append(end_idx - 2)       \n",
        "      elif context[start_idx + 2:end_idx + 2] == gold_text:\n",
        "        df.iloc[i, 5] = start_idx + 2\n",
        "        end_idx_list.append(end_idx + 2)  \n",
        "   \n",
        "      else:\n",
        "        #print(i)\n",
        "        print(context[start_idx:end_idx], gold_text)\n",
        "\n",
        "    else:\n",
        "      df.iloc[i, 5] = 0\n",
        "      end_idx_list.append(0)\n",
        "\n",
        "  df['Answer_end'] = end_idx_list\n",
        "  return df\n",
        "\n",
        "df_preprocessed = add_end_idx(df.copy())\n",
        "df_preprocessed.head()"
      ],
      "metadata": {
        "id": "S_rHQbRIdVMq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "cdc844f9-cbf9-4d1a-e5e5-8d2de7823e8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Theme                                          Paragraph  \\\n",
              "0  Beyoncé  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
              "1  Beyoncé  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
              "2  Beyoncé  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
              "3  Beyoncé  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
              "4  Beyoncé  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
              "\n",
              "                                            Question  Answer_possible  \\\n",
              "0  When did Beyonce leave Destiny's Child and bec...             True   \n",
              "1      What album made her a worldwide known artist?             True   \n",
              "2             Who managed the Destiny's Child group?             True   \n",
              "3                     When did Beyoncé rise to fame?             True   \n",
              "4     What role did Beyoncé have in Destiny's Child?             True   \n",
              "\n",
              "           Answer_text Answer_start  Answer_end  \n",
              "0                 2003          526         530  \n",
              "1  Dangerously in Love          505         524  \n",
              "2       Mathew Knowles          360         374  \n",
              "3           late 1990s          276         286  \n",
              "4          lead singer          290         301  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-94142119-1905-4f20-a507-5a06c9b490d8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Theme</th>\n",
              "      <th>Paragraph</th>\n",
              "      <th>Question</th>\n",
              "      <th>Answer_possible</th>\n",
              "      <th>Answer_text</th>\n",
              "      <th>Answer_start</th>\n",
              "      <th>Answer_end</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
              "      <td>True</td>\n",
              "      <td>2003</td>\n",
              "      <td>526</td>\n",
              "      <td>530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>What album made her a worldwide known artist?</td>\n",
              "      <td>True</td>\n",
              "      <td>Dangerously in Love</td>\n",
              "      <td>505</td>\n",
              "      <td>524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>Who managed the Destiny's Child group?</td>\n",
              "      <td>True</td>\n",
              "      <td>Mathew Knowles</td>\n",
              "      <td>360</td>\n",
              "      <td>374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>When did Beyoncé rise to fame?</td>\n",
              "      <td>True</td>\n",
              "      <td>late 1990s</td>\n",
              "      <td>276</td>\n",
              "      <td>286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>What role did Beyoncé have in Destiny's Child?</td>\n",
              "      <td>True</td>\n",
              "      <td>lead singer</td>\n",
              "      <td>290</td>\n",
              "      <td>301</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-94142119-1905-4f20-a507-5a06c9b490d8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-94142119-1905-4f20-a507-5a06c9b490d8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-94142119-1905-4f20-a507-5a06c9b490d8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "flJWqeza88om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data loading testing codes"
      ],
      "metadata": {
        "id": "v1Qqa6-_jWE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_answers = [{'text': text, 'answer_start': start, 'answer_end': end} \\\n",
        "                 for text, start, end in zip(df_preprocessed['Answer_text'], df_preprocessed['Answer_start'], df_preprocessed['Answer_end'])]"
      ],
      "metadata": {
        "id": "3lVL8OZ-ilXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "train_contexts = df['Paragraph']\n",
        "train_questions = df['Question']"
      ],
      "metadata": {
        "id": "myqxlMmjqjPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings = tokenizer(list(train_contexts), list(train_questions), truncation=True, padding=True)"
      ],
      "metadata": {
        "id": "pT3dPOvqnVyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "# tokenizer.model_max_length = 512\n",
        "\n",
        "# encodings = tokenizer(list(train_contexts[0:2]), list(train_questions[0:2]), truncation=True, padding=True)"
      ],
      "metadata": {
        "id": "z5hxnQxTAiTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.model_max_length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNJk2N-iXnk-",
        "outputId": "47b01586-f514-4b44-d19e-22ee424f851c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "512"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_token_positions(encodings, answers):\n",
        "  start_positions = []\n",
        "  end_positions = []\n",
        "  count = 0\n",
        "  for i in range(len(answers)):\n",
        "    start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
        "    end_positions.append(encodings.char_to_token(i, answers[i]['answer_end']))\n",
        "\n",
        "    # if start position is None, the answer passage has been truncated\n",
        "    if start_positions[-1] is None:\n",
        "      start_positions[-1] = tokenizer.model_max_length\n",
        "\n",
        "    # if end position is None, the 'char_to_token' function points to the space before the correct token - > add + 1\n",
        "    if end_positions[-1] is None:\n",
        "      end_positions[-1] = encodings.char_to_token(i, answers[i]['answer_end'] + 1)\n",
        "\n",
        "    if end_positions[-1] is None:\n",
        "      end_positions[-1] = encodings.char_to_token(i, answers[i]['answer_end'] - 1)\n",
        "        \n",
        "    if end_positions[-1] is None:\n",
        "      end_positions[-1] = tokenizer.model_max_length\n",
        "      print(start_positions[-1], end_positions[-1], answers[i]['text'],'====>', tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(encodings['input_ids'][i][start_positions[-1]: end_positions[-1]])))\n",
        "  \n",
        "\n",
        "  encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
        "\n",
        "add_token_positions(train_encodings, train_answers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfGJ80dQU1VK",
        "outputId": "a1473b64-3848-4a40-b9f1-4f57acd5ec04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "512 512 Skyfall ====> \n",
            "512 512 limited to a few hundred vinyl pressings ====> \n",
            "512 512 three-step ====> \n",
            "512 512 25 °C ====> \n",
            "512 512 80 °C (176 °F) ====> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utils"
      ],
      "metadata": {
        "id": "CsT2oGoVvooq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text(s):\n",
        "  \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n",
        "  import string, re\n",
        "  def remove_articles(text):\n",
        "    regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
        "    return re.sub(regex, \" \", text)\n",
        "  def white_space_fix(text):\n",
        "    return \" \".join(text.split())\n",
        "  def remove_punc(text):\n",
        "    exclude = set(string.punctuation)\n",
        "    return \"\".join(ch for ch in text if ch not in exclude)\n",
        "  def lower(text):\n",
        "    return text.lower()\n",
        "\n",
        "  return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def exact_match(prediction, truth):\n",
        "    return bool(normalize_text(prediction) == normalize_text(truth))\n",
        "\n",
        "def compute_f1(prediction, truth):\n",
        "  pred_tokens = normalize_text(prediction).split()\n",
        "  truth_tokens = normalize_text(truth).split()\n",
        "  \n",
        "  # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
        "  if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
        "    return int(pred_tokens == truth_tokens)\n",
        "  \n",
        "  common_tokens = set(pred_tokens) & set(truth_tokens)\n",
        "  \n",
        "  # if there are no common tokens then f1 = 0\n",
        "  if len(common_tokens) == 0:\n",
        "    return 0\n",
        "  \n",
        "  prec = len(common_tokens) / len(pred_tokens)\n",
        "  rec = len(common_tokens) / len(truth_tokens)\n",
        "  \n",
        "  return round(2 * (prec * rec) / (prec + rec), 2)\n",
        "\n",
        "\n",
        "\n",
        "def compute_f1_batch(outputs, batch, tokenizer):\n",
        "  answer_start = outputs[1].argmax(dim=1)  \n",
        "  answer_end = outputs[2].argmax(dim=1) \n",
        "\n",
        "  #print(answer_start.size(), answer_end.size())\n",
        "    \n",
        "  truths = [tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input[start:end])) \\\n",
        "                                   for input, start, end in zip(batch['input_ids'].tolist(), batch['start_positions'].tolist(), batch['end_positions'].tolist())]\n",
        "\n",
        "  #print(truths)\n",
        "\n",
        "  predictions = [tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input[start:end])) \\\n",
        "                                   for input, start, end in zip(batch['input_ids'].tolist(), answer_start.tolist(), answer_end.tolist())]\n",
        "\n",
        "  f1_acc = 0\n",
        "  for pred, truth in zip(predictions, truths):\n",
        "    f1_acc += compute_f1(pred, truth)\n",
        "\n",
        "  return round(f1_acc/len(truths), 3)\n",
        "\n"
      ],
      "metadata": {
        "id": "8Yma2MZXvqJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loader"
      ],
      "metadata": {
        "id": "tmBKYmM9jTSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDatset(torch.utils.data.Dataset):\n",
        "  def __init__(self, df, tokenizer):\n",
        "    self.data = df\n",
        "    self.tokenizer = tokenizer\n",
        "    \n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def get_answers(self, idx):\n",
        "    answer_text = self.data.loc[idx]['Answer_text']\n",
        "    answer_start = self.data.loc[idx]['Answer_start']\n",
        "    answer_end = self.data.loc[idx]['Answer_end']\n",
        "    is_impossibles = 0.0 if not self.data.loc[idx]['Answer_possible'] else 1.0\n",
        "    return {'text': answer_text, 'answer_start': answer_start, 'answer_end': answer_end, 'is_impossibles':is_impossibles}\n",
        "\n",
        "  def add_token_positions(self, encodings, answers):\n",
        "    start_positions = encodings.char_to_token(answers['answer_start'])\n",
        "    end_positions = encodings.char_to_token(answers['answer_end'])\n",
        "\n",
        "    # if start position is None, the answer passage has been truncated\n",
        "    if start_positions is None:\n",
        "      start_positions = tokenizer.model_max_length\n",
        "\n",
        "    # if end position is None, the 'char_to_token' function points to the space before the correct token - > add + 1\n",
        "    if end_positions is None:\n",
        "      end_positions = encodings.char_to_token(answers['answer_end'] + 1)\n",
        "      \n",
        "    if end_positions is None:\n",
        "      end_positions = encodings.char_to_token(answers['answer_end'] - 1)\n",
        "\n",
        "    if end_positions is None:\n",
        "      end_positions = tokenizer.model_max_length\n",
        "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions, 'is_impossibles':answers['is_impossibles']})\n",
        "    return encodings\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    #print(idx)\n",
        "    contexts = self.data.loc[idx]['Paragraph']\n",
        "    questions = self.data.loc[idx]['Question']\n",
        "    encodings = self.tokenizer(contexts, questions, max_length = 512, truncation=True, padding='max_length')\n",
        "    answers = self.get_answers(idx)\n",
        "    encodings = self.add_token_positions(encodings, answers) \n",
        "    return {key: torch.tensor(val) for key, val in encodings.items()}"
      ],
      "metadata": {
        "id": "q1H3PmB00TRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "RMW9cgMtJCXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BertForQuestionAnswering(BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super(BertForQuestionAnswering, self).__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.bert = BertModel(config)\n",
        "        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "        self.has_ans = nn.Sequential(\n",
        "            nn.Dropout(config.hidden_dropout_prob),\n",
        "            nn.Linear(config.hidden_size, 2)\n",
        "        )\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None,\n",
        "                start_positions=None, end_positions=None, is_impossibles=None):\n",
        "\n",
        "        outputs = self.bert(input_ids,\n",
        "                            attention_mask=attention_mask,\n",
        "                            token_type_ids=token_type_ids,\n",
        "                            position_ids=position_ids,\n",
        "                            head_mask=head_mask,\n",
        "                            inputs_embeds=inputs_embeds)\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "\n",
        "        logits = self.qa_outputs(sequence_output)\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "        start_logits = start_logits.squeeze(-1)\n",
        "        end_logits = end_logits.squeeze(-1)\n",
        "        # print(start_logits.size())\n",
        "        first_word = sequence_output[:, 0, :]\n",
        "        has_logits = self.has_ans(first_word)\n",
        "        # print(has_logits.size())\n",
        "        if start_positions is not None and end_positions is not None and is_impossibles is not None:\n",
        "            # If we are on multi-GPU, split add a dimension\n",
        "            if len(start_positions.size()) > 1:\n",
        "                start_positions = start_positions.squeeze(-1)\n",
        "            if len(end_positions.size()) > 1:\n",
        "                end_positions = end_positions.squeeze(-1)\n",
        "            if len(is_impossibles.size()) > 1:\n",
        "                is_impossibles = is_impossibles.squeeze(-1)\n",
        "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
        "            ignored_index = start_logits.size(1)\n",
        "            start_positions.clamp_(0, ignored_index)\n",
        "            end_positions.clamp_(0, ignored_index)\n",
        "            is_impossibles.clamp_(0, ignored_index)\n",
        "\n",
        "            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n",
        "            start_loss = loss_fct(start_logits, start_positions)\n",
        "            end_loss = loss_fct(end_logits, end_positions)\n",
        "            span_loss = start_loss + end_loss\n",
        "\n",
        "            # Internal Front Verification (I-FV)\n",
        "            # alpha1 == 1.0, alpha2 == 0.5\n",
        "            choice_loss = loss_fct(has_logits, is_impossibles.long())\n",
        "            total_loss = 1.0 * span_loss + 0.5 * choice_loss\n",
        "\n",
        "\n",
        "\n",
        "            outputs = (start_logits, end_logits, has_logits) + outputs[2:]\n",
        "\n",
        "        return (total_loss,) + outputs  # (loss), start_logits, end_logits, has_logits, (hidden_states), (attentions)"
      ],
      "metadata": {
        "id": "CksibrWeuvzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aCjznPRFm-Hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split data to train and validation"
      ],
      "metadata": {
        "id": "6Ik8tn4Qzrwr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Split"
      ],
      "metadata": {
        "id": "MKZQ-OpuIgDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, val_df = train_test_split(df_preprocessed, test_size = 0.2)\n",
        "\n",
        "train_df.reset_index(inplace=True)\n",
        "val_df.reset_index(inplace=True)\n",
        "train_df.head()'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "yGGnLX977m4z",
        "outputId": "992b32e1-2e1b-46b1-977e-6dfaf6d5aa7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'from sklearn.model_selection import train_test_split\\n\\ntrain_df, val_df = train_test_split(df_preprocessed, test_size = 0.2)\\n\\ntrain_df.reset_index(inplace=True)\\nval_df.reset_index(inplace=True)\\ntrain_df.head()'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = df_preprocessed[df_preprocessed['Theme'] != 'Hunting']\n",
        "train_df.reset_index(inplace=True)\n",
        "len(train_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9VaDL3wJGZN",
        "outputId": "219a6ae7-15ce-4fb3-d1d1-846516f4111b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "74680"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_df = df_preprocessed[df_preprocessed['Theme'] == 'Hunting']\n",
        "val_df.reset_index(inplace=True)\n",
        "len(val_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdQD_ciCJuHd",
        "outputId": "5dec2afa-bb8a-4aa0-8fbc-b2e2c494549d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "375"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training"
      ],
      "metadata": {
        "id": "qZsFkAzVzmw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model = 'bert-base-uncased'\n",
        "config = BertConfig.from_pretrained(bert_model)\n",
        "config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJLKPk9y0rVv",
        "outputId": "98859559-fda7-445c-8c22-8f72c16bb87c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertConfig {\n",
              "  \"architectures\": [\n",
              "    \"BertForMaskedLM\"\n",
              "  ],\n",
              "  \"attention_probs_dropout_prob\": 0.1,\n",
              "  \"classifier_dropout\": null,\n",
              "  \"gradient_checkpointing\": false,\n",
              "  \"hidden_act\": \"gelu\",\n",
              "  \"hidden_dropout_prob\": 0.1,\n",
              "  \"hidden_size\": 768,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 3072,\n",
              "  \"layer_norm_eps\": 1e-12,\n",
              "  \"max_position_embeddings\": 512,\n",
              "  \"model_type\": \"bert\",\n",
              "  \"num_attention_heads\": 12,\n",
              "  \"num_hidden_layers\": 12,\n",
              "  \"pad_token_id\": 0,\n",
              "  \"position_embedding_type\": \"absolute\",\n",
              "  \"transformers_version\": \"4.25.1\",\n",
              "  \"type_vocab_size\": 2,\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 30522\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained(bert_model)\n",
        "qa = BertForQuestionAnswering(config)\n",
        "model = qa.from_pretrained(bert_model)"
      ],
      "metadata": {
        "id": "EIRFanO9vuuv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13d07779-b50b-465e-ea7a-1f4ef1f13599"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias', 'has_ans.1.weight', 'has_ans.1.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"from transformers import BertForQuestionAnswering\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "model = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "5jPMMGhJnJKR",
        "outputId": "9013d05f-27fc-45ff-8919-81b4ac50c7a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'from transformers import BertForQuestionAnswering\\ntokenizer = BertTokenizerFast.from_pretrained(\\'bert-base-uncased\\')\\nmodel = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check on the available device - use GPU\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "metadata": {
        "id": "hHjJUIpAv1bK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 2\n",
        "\n",
        "train_data = CustomDatset(train_df, tokenizer)\n",
        "val_data = CustomDatset(val_df, tokenizer)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=8, shuffle=True)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_data, batch_size=4)\n",
        "\n",
        "optim = transformers.AdamW(model.parameters(), lr=2e-5, weight_decay = 0.01, no_deprecation_warning=True)\n",
        "\n",
        "# load model if exist, else comment the line\n",
        "# model = torch.load(\"/content/drive/MyDrive/QA/Models/bert_model_2.pth\", map_location=device)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "  model.train()\n",
        "  loop = tqdm(train_dataloader, leave=True)\n",
        "  \n",
        "  train_loss = []\n",
        "  train_f1 = []\n",
        "  for batch in loop:\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    start_positions = batch['start_positions'].to(device)\n",
        "    end_positions = batch['end_positions'].to(device)\n",
        "    is_impossibles = batch['is_impossibles'].to(device)\n",
        "    outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions, is_impossibles=is_impossibles)\n",
        "    loss = outputs[0]\n",
        "    optim.zero_grad()\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "    # F1 calculation\n",
        "    f1 = compute_f1_batch(outputs, batch, tokenizer)\n",
        "\n",
        "    train_loss.append(loss)\n",
        "    train_f1.append(f1)\n",
        "\n",
        "    loop.set_description(f'Epoch {epoch+1} Training')\n",
        "    loop.set_postfix(loss=loss.item(), F1=f1)\n",
        "\n",
        "\n",
        "  # saving the model \n",
        "  model_path = f\"/content/drive/MyDrive/QA/Models/bert_model_{epoch+1}.pth\"\n",
        "  # torch.save(model, model_path)\n",
        "\n",
        "  # validation\n",
        "  model.eval()\n",
        "  loop = tqdm(val_dataloader, leave=True)\n",
        "  val_f1 = []\n",
        "  val_loss = []\n",
        "  for batch in loop:\n",
        "    with torch.no_grad():\n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      attention_mask = batch['attention_mask'].to(device)\n",
        "      start_positions = batch['start_positions'].to(device)\n",
        "      end_positions = batch['end_positions'].to(device)\n",
        "      outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
        "      loss = outputs[0]\n",
        "      \n",
        "      # F1 calculation\n",
        "      f1 = compute_f1_batch(outputs, batch, tokenizer)\n",
        "\n",
        "      val_loss.append(loss)\n",
        "      val_f1.append(f1)\n",
        "\n",
        "      loop.set_description(f'Epoch {epoch+1} Validation')\n",
        "      loop.set_postfix(loss=loss.item(), F1=f1)\n",
        "\n",
        "  print(f'\\nEnd of epoch {epoch+1}|Training Loss: {sum(train_loss)/len(train_loss):.3f} F1 Score: {sum(train_f1)/len(train_f1):.3f}\\\n",
        "  |Validation Loss: {sum(val_loss)/len(val_loss):.3f} F1 Score: {sum(val_f1)/len(val_f1):.3f}')"
      ],
      "metadata": {
        "id": "_BbLEQ6Pv63g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "8ddffb42-79eb-4d28-f1f3-b05a5c3ecaf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 Training:   4%|▍         | 108/2500 [01:22<30:31,  1.31it/s, F1=0.375, loss=7.15]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-22826ad9b729>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}