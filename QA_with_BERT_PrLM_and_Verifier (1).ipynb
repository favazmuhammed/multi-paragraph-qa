{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t6Ix2N9xP3Tq","outputId":"dadda2c2-bbaf-42d8-e1f8-52139d4316ed"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /home/mlg2/anaconda3/lib/python3.9/site-packages (4.25.1)\n","Requirement already satisfied: packaging>=20.0 in /home/mlg2/anaconda3/lib/python3.9/site-packages (from transformers) (21.3)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/mlg2/anaconda3/lib/python3.9/site-packages (from transformers) (0.13.2)\n","Requirement already satisfied: pyyaml>=5.1 in /home/mlg2/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n","Requirement already satisfied: tqdm>=4.27 in /home/mlg2/anaconda3/lib/python3.9/site-packages (from transformers) (4.64.1)\n","Requirement already satisfied: numpy>=1.17 in /home/mlg2/anaconda3/lib/python3.9/site-packages (from transformers) (1.21.5)\n","Requirement already satisfied: requests in /home/mlg2/anaconda3/lib/python3.9/site-packages (from transformers) (2.28.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /home/mlg2/anaconda3/lib/python3.9/site-packages (from transformers) (0.11.1)\n","Requirement already satisfied: regex!=2019.12.17 in /home/mlg2/anaconda3/lib/python3.9/site-packages (from transformers) (2022.7.9)\n","Requirement already satisfied: filelock in /home/mlg2/anaconda3/lib/python3.9/site-packages (from transformers) (3.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/mlg2/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.3.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/mlg2/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: charset-normalizer<3,>=2 in /home/mlg2/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /home/mlg2/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2022.9.14)\n","Requirement already satisfied: idna<4,>=2.5 in /home/mlg2/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.3)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/mlg2/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.11)\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t1n4nrhTp5rw"},"outputs":[],"source":["import torch\n","from torch import nn\n","from torch.nn import CrossEntropyLoss, MSELoss\n","import pandas as pd\n","import transformers\n","from transformers import BertTokenizerFast\n","from transformers import BertModel, BertConfig, BertPreTrainedModel\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"dGV5nqML0TOB","outputId":"e716e3f4-94c4-4a23-9008-e6fe746bab88"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Theme</th>\n","      <th>Paragraph</th>\n","      <th>Question</th>\n","      <th>Answer_possible</th>\n","      <th>Answer_text</th>\n","      <th>Answer_start</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Beyoncé</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","      <td>When did Beyonce leave Destiny's Child and bec...</td>\n","      <td>True</td>\n","      <td>['2003']</td>\n","      <td>[526]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Beyoncé</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","      <td>What album made her a worldwide known artist?</td>\n","      <td>True</td>\n","      <td>['Dangerously in Love']</td>\n","      <td>[505]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Beyoncé</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","      <td>Who managed the Destiny's Child group?</td>\n","      <td>True</td>\n","      <td>['Mathew Knowles']</td>\n","      <td>[360]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Beyoncé</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","      <td>When did Beyoncé rise to fame?</td>\n","      <td>True</td>\n","      <td>['late 1990s']</td>\n","      <td>[276]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Beyoncé</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","      <td>What role did Beyoncé have in Destiny's Child?</td>\n","      <td>True</td>\n","      <td>['lead singer']</td>\n","      <td>[290]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     Theme                                          Paragraph  \\\n","0  Beyoncé  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n","1  Beyoncé  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n","2  Beyoncé  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n","3  Beyoncé  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n","4  Beyoncé  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n","\n","                                            Question  Answer_possible  \\\n","0  When did Beyonce leave Destiny's Child and bec...             True   \n","1      What album made her a worldwide known artist?             True   \n","2             Who managed the Destiny's Child group?             True   \n","3                     When did Beyoncé rise to fame?             True   \n","4     What role did Beyoncé have in Destiny's Child?             True   \n","\n","               Answer_text Answer_start  \n","0                 ['2003']        [526]  \n","1  ['Dangerously in Love']        [505]  \n","2       ['Mathew Knowles']        [360]  \n","3           ['late 1990s']        [276]  \n","4          ['lead singer']        [290]  "]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv(\"train_data.csv\")\n","df.drop(\"Unnamed: 0\",axis=1, inplace=True)\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"64aysiZSIWE9","outputId":"3e45995c-d871-4e27-fb94-6f8574369859"},"outputs":[{"data":{"text/plain":["array(['Beyoncé', 'Frédéric_Chopin',\n","       'Sino-Tibetan_relations_during_the_Ming_dynasty',\n","       'The_Legend_of_Zelda:_Twilight_Princess', 'Spectre_(2015_film)',\n","       'New_York_City', 'To_Kill_a_Mockingbird', 'Solar_energy',\n","       'Kanye_West', 'Buddhism', 'American_Idol', 'Dog',\n","       '2008_Summer_Olympics_torch_relay', 'Genome',\n","       'Comprehensive_school', 'Republic_of_the_Congo', 'Prime_minister',\n","       'Institute_of_technology', 'Dutch_Republic', 'Symbiosis',\n","       'Iranian_languages', 'Lighting',\n","       'Separation_of_powers_under_the_United_States_Constitution',\n","       'Architecture', 'Southern_Europe', 'BBC_Television',\n","       'Arnold_Schwarzenegger', 'Plymouth', 'Christian',\n","       'Sony_Music_Entertainment', 'Oklahoma_City', 'Hunter-gatherer',\n","       'United_Nations_Population_Fund',\n","       'Russian_Soviet_Federative_Socialist_Republic',\n","       'Alexander_Graham_Bell', 'Internet_service_provider', 'Comics',\n","       'Saint_Helena', 'Aspirated_consonant', 'Hydrogen', 'Space_Race',\n","       'BeiDou_Navigation_Satellite_System', 'Canon_law',\n","       'Communications_in_Somalia', 'Boston', 'Universal_Studios',\n","       'Estonian_language', 'Daylight_saving_time',\n","       'Royal_Institute_of_British_Architects',\n","       'National_Archives_and_Records_Administration', 'Tristan_da_Cunha',\n","       'University_of_Kansas', 'Arena_Football_League', 'Bern',\n","       'Westminster_Abbey', 'Political_corruption', 'Classical_music',\n","       'Slavs', 'Treaty', 'Josip_Broz_Tito', 'Marshall_Islands',\n","       'Szlachta', 'Virgil', 'Alps', 'Gene', 'Guinea-Bissau',\n","       'List_of_numbered_streets_in_Manhattan', 'Brain', 'Near_East',\n","       'Zhejiang', 'Ministry_of_Defence_(United_Kingdom)',\n","       'High-definition_television', 'Wood', 'Somalis', 'Middle_Ages',\n","       'Phonology', 'Computer', 'Black_people', 'New_Delhi',\n","       'Bird_migration', 'Atlantic_City,_New_Jersey', 'MP3',\n","       'House_music', 'Letter_case', 'Chihuahua_(state)', 'Pitch_(music)',\n","       'England_national_football_team', 'Houston', 'Copper',\n","       'Identity_(social_science)', 'Himachal_Pradesh', 'Communication',\n","       'Computer_security', 'Orthodox_Judaism', 'Animal', 'Beer',\n","       'Race_and_ethnicity_in_the_United_States_Census',\n","       'Imperial_College_London', 'Hanover', 'Emotion', 'Old_English',\n","       'Aircraft_carrier', 'Federal_Aviation_Administration',\n","       'Lancashire', 'Mesozoic', 'Videoconferencing',\n","       'Gregorian_calendar', 'Xbox_360',\n","       'Military_history_of_the_United_States', 'Infrared', 'ASCII',\n","       'Digestion', 'Gymnastics', 'FC_Barcelona', 'Melbourne',\n","       'John,_King_of_England', 'Macintosh', 'Valencia',\n","       'General_Electric', 'United_States_Army', 'Franco-Prussian_War',\n","       'Adolescence', 'Antarctica', 'Eritrea', 'Uranium',\n","       'Circadian_rhythm', 'Sexual_orientation', 'Dell',\n","       'Nintendo_Entertainment_System', 'Seattle', 'Memory',\n","       'Multiracial_American', 'Ashkenazi_Jews',\n","       'Pharmaceutical_industry', 'Umayyad_Caliphate', 'Asphalt',\n","       'Queen_Victoria', 'Israel', 'Hellenistic_period',\n","       'Bill_%26_Melinda_Gates_Foundation', 'Dutch_language',\n","       'Buckingham_Palace', 'Incandescent_light_bulb', 'Arsenal_F.C.',\n","       'Chicago_Cubs', 'Korean_War', 'Copyright_infringement', 'Greece',\n","       'Royal_Dutch_Shell', 'Mammal', 'East_India_Company', 'Hokkien',\n","       'Professional_wrestling', 'Film_speed', 'Mexico_City', 'Napoleon',\n","       'Germans', 'Southeast_Asia', 'Brigham_Young_University',\n","       'Intellectual_property', 'Florida', 'Queen_(band)',\n","       'Presbyterianism', 'Thuringia', 'Predation', 'British_Empire',\n","       'Botany', 'Madonna_(entertainer)', 'Law_of_the_United_States',\n","       'Myanmar', 'Jews', 'Cotton', 'Data_compression',\n","       'The_Sun_(United_Kingdom)', 'Pesticide', 'Somerset',\n","       'Yale_University', 'Late_Middle_Ages', 'Ann_Arbor,_Michigan',\n","       'Gothic_architecture', 'Cubism', 'Political_philosophy',\n","       'Norfolk_Island', 'Edmund_Burke', 'Samoa', 'Pope_Paul_VI',\n","       'Switzerland', 'Mali', 'Raleigh,_North_Carolina', 'Crimean_War',\n","       'Nonprofit_organization', 'Literature', 'Avicenna', 'Nigeria',\n","       'Molotov%E2%80%93Ribbentrop_Pact', 'History_of_science', 'Digimon',\n","       'Glacier', 'Affirmative_action_in_the_United_States', 'FA_Cup',\n","       'New_Haven,_Connecticut', 'Alsace', 'Carnival', 'Baptists',\n","       'Child_labour', 'Dissolution_of_the_Soviet_Union',\n","       'Crucifixion_of_Jesus', 'Supreme_court', 'Textual_criticism',\n","       'Gramophone_record', 'Turner_Classic_Movies', 'Hindu_philosophy',\n","       'A_cappella', 'Dominican_Order', 'Eton_College', 'Cork_(city)',\n","       'Galicia_(Spain)', 'USB', 'Sichuan', 'Unicode', 'Detroit',\n","       'London', 'Culture', 'Sahara', 'Rule_of_law', 'Exhibition_game',\n","       'Northwestern_University', 'Strasbourg', 'History_of_India',\n","       'Gamal_Abdel_Nasser', 'Pope_John_XXIII', 'Time',\n","       'St._John%27s,_Newfoundland_and_Labrador', 'John_von_Neumann',\n","       'PlayStation_3', 'Royal_assent', 'Central_African_Republic',\n","       'Asthma', 'LaserDisc', 'George_VI', 'Federalism', 'Annelid',\n","       'War_on_Terror', 'Labour_Party_(UK)', 'Estonia', 'Alaska',\n","       'Mandolin', 'Insect', 'Race_(human_categorization)', 'Paris',\n","       'Apollo', 'United_States_presidential_election,_2004',\n","       'Liberal_Party_of_Australia', 'Samurai', 'Software_testing',\n","       'States_of_Germany', 'Glass', 'Renewable_energy_commercialization',\n","       'Palermo', 'Green', 'Neoclassical_architecture', 'Serbo-Croatian',\n","       'CBC_Television', 'IBM', 'Energy', 'East_Prussia',\n","       'Ottoman_Empire', 'Philosophy_of_space_and_time', 'Neolithic',\n","       'Friedrich_Hayek', 'Diarrhea', 'Madrasa', 'Miami', 'Philadelphia',\n","       'John_Kerry', 'Rajasthan', 'Guam', 'Empiricism', 'Idealism',\n","       'Czech_language', 'Education', 'Tennessee', 'Canadian_football',\n","       'Seven_Years%27_War', 'Cyprus', 'Steven_Spielberg', 'Elevator',\n","       'Railway_electrification_system',\n","       'Spanish_language_in_the_United_States',\n","       'Charleston,_South_Carolina', 'The_Blitz', 'Vacuum', 'Quran',\n","       'Geography_of_the_United_States', 'Compact_disc', 'Modern_history',\n","       'Flowering_plant', 'Hyderabad', 'Santa_Monica,_California', 'Pain',\n","       'Database', 'Tucson,_Arizona', 'Bacteria', 'Printed_circuit_board',\n","       'Greeks', 'Premier_League', 'Roman_Republic', 'Pacific_War',\n","       'San_Diego', 'Iran', 'British_Isles', 'Association_football',\n","       'Georgian_architecture', 'Liberia', 'Alfred_North_Whitehead',\n","       'Antibiotics', 'Windows_8', 'Swaziland', 'Translation', 'Airport',\n","       'Super_Nintendo_Entertainment_System', 'Sumer', 'Tuvalu',\n","       'Namibia', 'Russian_language', 'United_States_Air_Force',\n","       'Light-emitting_diode', 'Great_power', 'Bird', 'Qing_dynasty',\n","       'Indigenous_peoples_of_the_Americas', 'Red', 'Mosaic',\n","       'University', 'Religion_in_ancient_Rome', 'YouTube',\n","       'Separation_of_church_and_state_in_the_United_States',\n","       'Bras%C3%ADlia', 'Economy_of_Greece',\n","       'Party_leaders_of_the_United_States_House_of_Representatives',\n","       'Armenians', 'Jehovah%27s_Witnesses', 'Dwight_D._Eisenhower',\n","       'The_Bronx', 'Financial_crisis_of_2007%E2%80%9308', 'Portugal',\n","       'Humanism', 'Geological_history_of_Earth', 'Police', 'Genocide',\n","       'Saint_Barth%C3%A9lemy', 'Tajikistan', 'University_of_Notre_Dame',\n","       'Anthropology', 'Montana', 'Punjab,_Pakistan', 'Infection',\n","       'Hunting', 'Kathmandu', 'Myocardial_infarction', 'Matter'],\n","      dtype=object)"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# unique themese in the dataset\n","df.Theme.unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"wpF0Malcadlg","outputId":"785db26e-419e-4645-b3ae-8cd4feca4ae4"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Theme</th>\n","      <th>Paragraph</th>\n","      <th>Question</th>\n","      <th>Answer_possible</th>\n","      <th>Answer_text</th>\n","      <th>Answer_start</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Beyoncé</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","      <td>When did Beyonce leave Destiny's Child and bec...</td>\n","      <td>True</td>\n","      <td>2003</td>\n","      <td>526</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Beyoncé</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","      <td>What album made her a worldwide known artist?</td>\n","      <td>True</td>\n","      <td>Dangerously in Love</td>\n","      <td>505</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Beyoncé</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","      <td>Who managed the Destiny's Child group?</td>\n","      <td>True</td>\n","      <td>Mathew Knowles</td>\n","      <td>360</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Beyoncé</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","      <td>When did Beyoncé rise to fame?</td>\n","      <td>True</td>\n","      <td>late 1990s</td>\n","      <td>276</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Beyoncé</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","      <td>What role did Beyoncé have in Destiny's Child?</td>\n","      <td>True</td>\n","      <td>lead singer</td>\n","      <td>290</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     Theme                                          Paragraph  \\\n","0  Beyoncé  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n","1  Beyoncé  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n","2  Beyoncé  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n","3  Beyoncé  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n","4  Beyoncé  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n","\n","                                            Question  Answer_possible  \\\n","0  When did Beyonce leave Destiny's Child and bec...             True   \n","1      What album made her a worldwide known artist?             True   \n","2             Who managed the Destiny's Child group?             True   \n","3                     When did Beyoncé rise to fame?             True   \n","4     What role did Beyoncé have in Destiny's Child?             True   \n","\n","           Answer_text Answer_start  \n","0                 2003          526  \n","1  Dangerously in Love          505  \n","2       Mathew Knowles          360  \n","3           late 1990s          276  \n","4          lead singer          290  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# preprocessing answer text\n","df['Answer_text'] = df['Answer_text'].apply(lambda x: x.lstrip(\"[\").rstrip(\"]\").strip(\"'\").strip('''\"'''))\n","df['Answer_text'] = df['Answer_text'].apply(lambda x: x.replace(\"\\\\\",\"\"))\n","df['Answer_start'] = df['Answer_start'].apply(lambda x: x.lstrip('[').rstrip(']'))\n","df.iloc[37668, 4] = df.iloc[37668, 4].replace(\"ufeff\", \"\")\n","df.iloc[37668, 1] = df.iloc[37668, 1].replace(\"\\ufeff\", \"\")\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"S_rHQbRIdVMq","outputId":"cdc844f9-cbf9-4d1a-e5e5-8d2de7823e8d"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Theme</th>\n","      <th>Paragraph</th>\n","      <th>Question</th>\n","      <th>Answer_possible</th>\n","      <th>Answer_text</th>\n","      <th>Answer_start</th>\n","      <th>Answer_end</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Beyoncé</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","      <td>When did Beyonce leave Destiny's Child and bec...</td>\n","      <td>True</td>\n","      <td>2003</td>\n","      <td>526</td>\n","      <td>530</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Beyoncé</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","      <td>What album made her a worldwide known artist?</td>\n","      <td>True</td>\n","      <td>Dangerously in Love</td>\n","      <td>505</td>\n","      <td>524</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Beyoncé</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","      <td>Who managed the Destiny's Child group?</td>\n","      <td>True</td>\n","      <td>Mathew Knowles</td>\n","      <td>360</td>\n","      <td>374</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Beyoncé</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","      <td>When did Beyoncé rise to fame?</td>\n","      <td>True</td>\n","      <td>late 1990s</td>\n","      <td>276</td>\n","      <td>286</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Beyoncé</td>\n","      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n","      <td>What role did Beyoncé have in Destiny's Child?</td>\n","      <td>True</td>\n","      <td>lead singer</td>\n","      <td>290</td>\n","      <td>301</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     Theme                                          Paragraph  \\\n","0  Beyoncé  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n","1  Beyoncé  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n","2  Beyoncé  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n","3  Beyoncé  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n","4  Beyoncé  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n","\n","                                            Question  Answer_possible  \\\n","0  When did Beyonce leave Destiny's Child and bec...             True   \n","1      What album made her a worldwide known artist?             True   \n","2             Who managed the Destiny's Child group?             True   \n","3                     When did Beyoncé rise to fame?             True   \n","4     What role did Beyoncé have in Destiny's Child?             True   \n","\n","           Answer_text Answer_start  Answer_end  \n","0                 2003          526         530  \n","1  Dangerously in Love          505         524  \n","2       Mathew Knowles          360         374  \n","3           late 1990s          276         286  \n","4          lead singer          290         301  "]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["def add_end_idx(df):\n","  end_idx_list = []\n","  for i in range(len(df)):\n","    gold_text = df.iloc[i, 4]\n","    context = df.iloc[i,1]\n","\n","    if df.iloc[i,3] == True:\n","      start_idx = int(df.iloc[i, 5])\n","      end_idx = start_idx + len(gold_text)\n","      \n","\n","      # sometimes squad answers are off by a character or two so we fix this\n","      if context[start_idx : end_idx] == gold_text:\n","        df.iloc[i, 5] = start_idx\n","        end_idx_list.append(end_idx)\n","      elif context[start_idx - 1:end_idx - 1] == gold_text:\n","        df.iloc[i, 5] = start_idx - 1\n","        end_idx_list.append(end_idx - 1) \n","      elif context[start_idx + 1:end_idx + 1] == gold_text:\n","        df.iloc[i, 5] = start_idx + 1\n","        end_idx_list.append(end_idx + 1)     \n","      elif context[start_idx - 2:end_idx - 2] == gold_text:\n","        df.iloc[i, 5] = start_idx - 2\n","        end_idx_list.append(end_idx - 2)       \n","      elif context[start_idx + 2:end_idx + 2] == gold_text:\n","        df.iloc[i, 5] = start_idx + 2\n","        end_idx_list.append(end_idx + 2)  \n","   \n","      else:\n","        #print(i)\n","        print(context[start_idx:end_idx], gold_text)\n","\n","    else:\n","      df.iloc[i, 5] = 0\n","      end_idx_list.append(0)\n","\n","  df['Answer_end'] = end_idx_list\n","  return df\n","\n","df_preprocessed = add_end_idx(df.copy())\n","df_preprocessed.head()"]},{"cell_type":"markdown","metadata":{"id":"CsT2oGoVvooq"},"source":["### Utils"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Yma2MZXvqJo"},"outputs":[],"source":["def normalize_text(s):\n","  \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n","  import string, re\n","  def remove_articles(text):\n","    regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n","    return re.sub(regex, \" \", text)\n","  def white_space_fix(text):\n","    return \" \".join(text.split())\n","  def remove_punc(text):\n","    exclude = set(string.punctuation)\n","    return \"\".join(ch for ch in text if ch not in exclude)\n","  def lower(text):\n","    return text.lower()\n","\n","  return white_space_fix(remove_articles(remove_punc(lower(s))))\n","\n","def exact_match(prediction, truth):\n","    return bool(normalize_text(prediction) == normalize_text(truth))\n","\n","def compute_f1(prediction, truth):\n","  pred_tokens = normalize_text(prediction).split()\n","  truth_tokens = normalize_text(truth).split()\n","  \n","  # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n","  if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n","    return int(pred_tokens == truth_tokens)\n","  \n","  common_tokens = set(pred_tokens) & set(truth_tokens)\n","  \n","  # if there are no common tokens then f1 = 0\n","  if len(common_tokens) == 0:\n","    return 0\n","  \n","  prec = len(common_tokens) / len(pred_tokens)\n","  rec = len(common_tokens) / len(truth_tokens)\n","  \n","  return round(2 * (prec * rec) / (prec + rec), 2)\n","\n","\n","\n","def compute_f1_batch(outputs, batch, tokenizer):\n","  answer_start = outputs[1].argmax(dim=1)  \n","  answer_end = outputs[2].argmax(dim=1) \n","\n","  #print(answer_start.size(), answer_end.size())\n","    \n","  truths = [tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input[start:end])) \\\n","                                   for input, start, end in zip(batch['input_ids'].tolist(), batch['start_positions'].tolist(), batch['end_positions'].tolist())]\n","\n","  #print(truths)\n","\n","  predictions = [tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input[start:end])) \\\n","                                   for input, start, end in zip(batch['input_ids'].tolist(), answer_start.tolist(), answer_end.tolist())]\n","\n","  f1_acc = 0\n","  for pred, truth in zip(predictions, truths):\n","    f1_acc += compute_f1(pred, truth)\n","\n","  return round(f1_acc/len(truths), 3)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"tmBKYmM9jTSy"},"source":["### Data Loader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q1H3PmB00TRZ"},"outputs":[],"source":["class CustomDatset(torch.utils.data.Dataset):\n","  def __init__(self, df, tokenizer):\n","    self.data = df\n","    self.tokenizer = tokenizer\n","    \n","\n","  def __len__(self):\n","    return len(self.data)\n","\n","  def get_answers(self, idx):\n","    answer_text = self.data.loc[idx]['Answer_text']\n","    answer_start = self.data.loc[idx]['Answer_start']\n","    answer_end = self.data.loc[idx]['Answer_end']\n","    is_impossibles = 0.0 if not self.data.loc[idx]['Answer_possible'] else 1.0\n","    return {'text': answer_text, 'answer_start': answer_start, 'answer_end': answer_end, 'is_impossibles':is_impossibles}\n","\n","  def add_token_positions(self, encodings, answers):\n","    start_positions = encodings.char_to_token(answers['answer_start'])\n","    end_positions = encodings.char_to_token(answers['answer_end'])\n","\n","    # if start position is None, the answer passage has been truncated\n","    if start_positions is None:\n","      start_positions = tokenizer.model_max_length\n","\n","    # if end position is None, the 'char_to_token' function points to the space before the correct token - > add + 1\n","    if end_positions is None:\n","      end_positions = encodings.char_to_token(answers['answer_end'] + 1)\n","      \n","    if end_positions is None:\n","      end_positions = encodings.char_to_token(answers['answer_end'] - 1)\n","\n","    if end_positions is None:\n","      end_positions = tokenizer.model_max_length\n","    encodings.update({'start_positions': start_positions, 'end_positions': end_positions, 'is_impossibles':answers['is_impossibles']})\n","    return encodings\n","\n","  def __getitem__(self, idx):\n","    #print(idx)\n","    contexts = self.data.loc[idx]['Paragraph']\n","    questions = self.data.loc[idx]['Question']\n","    encodings = self.tokenizer(contexts, questions, max_length = 512, truncation=True, padding='max_length')\n","    answers = self.get_answers(idx)\n","    encodings = self.add_token_positions(encodings, answers) \n","    return {key: torch.tensor(val) for key, val in encodings.items()}"]},{"cell_type":"markdown","metadata":{"id":"RMW9cgMtJCXj"},"source":["### Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CksibrWeuvzj"},"outputs":[],"source":["class BertForQuestionAnswering(BertPreTrainedModel):\n","    def __init__(self, config):\n","        super(BertForQuestionAnswering, self).__init__(config)\n","        self.num_labels = config.num_labels\n","\n","        self.bert = BertModel(config)\n","        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n","\n","        self.has_ans = nn.Sequential(\n","            nn.Dropout(config.hidden_dropout_prob),\n","            nn.Linear(config.hidden_size, 2)\n","        )\n","\n","        self.init_weights()\n","\n","    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, \n","                inputs_embeds=None,start_positions=None, end_positions=None, is_impossibles=None):\n","\n","        outputs = self.bert(input_ids,\n","                            attention_mask=attention_mask,\n","                            token_type_ids=token_type_ids,\n","                            position_ids=position_ids,\n","                            head_mask=head_mask,\n","                            inputs_embeds=inputs_embeds)\n","\n","        sequence_output = outputs[0]\n","\n","        logits = self.qa_outputs(sequence_output)\n","        start_logits, end_logits = logits.split(1, dim=-1)\n","        start_logits = start_logits.squeeze(-1)\n","        end_logits = end_logits.squeeze(-1)\n","        # print(start_logits.size())\n","        first_word = sequence_output[:, 0, :]\n","        has_logits = self.has_ans(first_word)\n","        # print(has_logits.size())\n","        if start_positions is not None and end_positions is not None and is_impossibles is not None:\n","            # If we are on multi-GPU, split add a dimension\n","            if len(start_positions.size()) > 1:\n","                start_positions = start_positions.squeeze(-1)\n","            if len(end_positions.size()) > 1:\n","                end_positions = end_positions.squeeze(-1)\n","            if len(is_impossibles.size()) > 1:\n","                is_impossibles = is_impossibles.squeeze(-1)\n","            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n","            ignored_index = start_logits.size(1)\n","            start_positions.clamp_(0, ignored_index)\n","            end_positions.clamp_(0, ignored_index)\n","            is_impossibles.clamp_(0, ignored_index)\n","\n","            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n","            start_loss = loss_fct(start_logits, start_positions)\n","            end_loss = loss_fct(end_logits, end_positions)\n","            span_loss = start_loss + end_loss\n","\n","            # Internal Front Verification (I-FV)\n","            # alpha1 == 1.0, alpha2 == 0.5\n","            choice_loss = loss_fct(has_logits, is_impossibles.long())\n","            total_loss = 1.0 * span_loss + 0.5 * choice_loss\n","\n","\n","\n","            outputs = (start_logits, end_logits, has_logits) + outputs[2:]\n","\n","        return (total_loss,) + outputs  # (loss), start_logits, end_logits, has_logits, (hidden_states), (attentions)"]},{"cell_type":"markdown","metadata":{"id":"6Ik8tn4Qzrwr"},"source":["### Split data to train and validation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b9VaDL3wJGZN","outputId":"219a6ae7-15ce-4fb3-d1d1-846516f4111b"},"outputs":[{"data":{"text/plain":["74680"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["train_df = df_preprocessed[df_preprocessed['Theme'] != 'Hunting']\n","train_df.reset_index(inplace=True)\n","len(train_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jdQD_ciCJuHd","outputId":"5dec2afa-bb8a-4aa0-8fbc-b2e2c494549d"},"outputs":[{"data":{"text/plain":["375"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["val_df = df_preprocessed[df_preprocessed['Theme'] == 'Hunting']\n","val_df.reset_index(inplace=True)\n","len(val_df)"]},{"cell_type":"markdown","metadata":{"id":"qZsFkAzVzmw3"},"source":["### Model Training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rJLKPk9y0rVv","outputId":"98859559-fda7-445c-8c22-8f72c16bb87c"},"outputs":[{"data":{"text/plain":["BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["bert_model = 'bert-base-uncased'\n","config = BertConfig.from_pretrained(bert_model)\n","config"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EIRFanO9vuuv","outputId":"13d07779-b50b-465e-ea7a-1f4ef1f13599"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['has_ans.1.weight', 'qa_outputs.bias', 'qa_outputs.weight', 'has_ans.1.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["tokenizer = BertTokenizerFast.from_pretrained(bert_model)\n","qa = BertForQuestionAnswering(config)\n","model = qa.from_pretrained(bert_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hHjJUIpAv1bK","outputId":"88c41514-c3d9-4416-d0bb-3ed7e8149467"},"outputs":[{"name":"stdout","output_type":"stream","text":["You are working on cuda\n"]}],"source":["# Check on the available device - use GPU\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","print(f\"You are working on {device}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":397},"id":"_BbLEQ6Pv63g","outputId":"8ddffb42-79eb-4d28-f1f3-b05a5c3ecaf2"},"outputs":[{"name":"stderr","output_type":"stream","text":["Epoch 1 Training: 100%|███████████████████████████████████████| 9335/9335 [1:03:29<00:00,  2.45it/s, F1=0.74, loss=2.23]\n","Epoch 1 Validation: 100%|██████████████████████████████████████████| 94/94 [00:06<00:00, 14.18it/s, F1=0.333, loss=5.06]\n"]},{"name":"stdout","output_type":"stream","text":["\n","End of epoch 1|Training Loss: 3.270 F1 Score:   0.558|Validation Loss: 2.091   F1 Score: 0.724\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2 Training: 100%|███████████████████████████████████████| 9335/9335 [1:03:35<00:00,  2.45it/s, F1=0.771, loss=1.7]\n","Epoch 2 Validation: 100%|██████████████████████████████████████████| 94/94 [00:06<00:00, 14.20it/s, F1=0.667, loss=1.31]\n"]},{"name":"stdout","output_type":"stream","text":["\n","End of epoch 2|Training Loss: 1.885 F1 Score:   0.749|Validation Loss: 2.164   F1 Score: 0.721\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3 Training: 100%|██████████████████████████████████████| 9335/9335 [1:03:33<00:00,  2.45it/s, F1=0.875, loss=1.05]\n","Epoch 3 Validation: 100%|██████████████████████████████████████████| 94/94 [00:06<00:00, 14.22it/s, F1=0.667, loss=3.63]\n"]},{"name":"stdout","output_type":"stream","text":["\n","End of epoch 3|Training Loss: 1.280 F1 Score:   0.836|Validation Loss: 1.949   F1 Score: 0.774\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4 Training: 100%|██████████████████████████████████████| 9335/9335 [1:03:33<00:00,  2.45it/s, F1=0.829, loss=1.09]\n","Epoch 4 Validation: 100%|██████████████████████████████████████████| 94/94 [00:06<00:00, 14.19it/s, F1=0.667, loss=1.07]\n"]},{"name":"stdout","output_type":"stream","text":["\n","End of epoch 4|Training Loss: 0.898 F1 Score:   0.885|Validation Loss: 2.230   F1 Score: 0.770\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5 Training: 100%|█████████████████████████████████████| 9335/9335 [1:03:34<00:00,  2.45it/s, F1=0.834, loss=0.845]\n","Epoch 5 Validation: 100%|██████████████████████████████████████████| 94/94 [00:06<00:00, 14.21it/s, F1=0.667, loss=3.02]\n"]},{"name":"stdout","output_type":"stream","text":["\n","End of epoch 5|Training Loss: 0.665 F1 Score:   0.915|Validation Loss: 2.635   F1 Score: 0.761\n"]}],"source":["N_EPOCHS = 5\n","\n","train_data = CustomDatset(train_df, tokenizer)\n","val_data = CustomDatset(val_df, tokenizer)\n","train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=8, shuffle=True)\n","val_dataloader = torch.utils.data.DataLoader(val_data, batch_size=4)\n","\n","optim = transformers.AdamW(model.parameters(), lr=2e-5, weight_decay = 0.01, no_deprecation_warning=True)\n","\n","# load model if exist, else comment the line\n","# model = torch.load(\"bert_encode_with_FIV_model_2.pth\", map_location=device)\n","\n","model.to(device)\n","\n","\n","for epoch in range(N_EPOCHS):\n","  model.train()\n","  loop = tqdm(train_dataloader, leave=True)\n","  \n","  train_loss = []\n","  train_f1 = []\n","  for batch in loop:\n","    input_ids = batch['input_ids'].to(device)\n","    attention_mask = batch['attention_mask'].to(device)\n","    start_positions = batch['start_positions'].to(device)\n","    end_positions = batch['end_positions'].to(device)\n","    is_impossibles = batch['is_impossibles'].to(device)\n","    \n","    outputs = model(input_ids, \n","                    attention_mask=attention_mask, \n","                    start_positions=start_positions, \n","                    end_positions=end_positions, \n","                    is_impossibles=is_impossibles)\n","    \n","    loss = outputs[0]\n","    optim.zero_grad()\n","    loss.backward()\n","    optim.step()\n","\n","    # F1 calculation\n","    f1 = compute_f1_batch(outputs, batch, tokenizer)\n","\n","    train_loss.append(loss)\n","    train_f1.append(f1)\n","    \n","    \n","\n","    loop.set_description(f'Epoch {epoch+1} Training')\n","    loop.set_postfix(loss=loss.item(), F1=f1)\n","\n","\n","  # saving the model \n","  model_path = f\"bert_encode_with_FIV_model_{epoch+1}.pth\"\n","  # torch.save(model, model_path)\n","\n","  # validation\n","  model.eval()\n","  loop = tqdm(val_dataloader, leave=True)\n","  val_f1 = []\n","  val_loss = []\n","  for batch in loop:\n","    with torch.no_grad():\n","      input_ids = batch['input_ids'].to(device)\n","      attention_mask = batch['attention_mask'].to(device)\n","      start_positions = batch['start_positions'].to(device)\n","      end_positions = batch['end_positions'].to(device)\n","      is_impossibles = batch['is_impossibles'].to(device)\n","    \n","      outputs = model(input_ids, \n","                      attention_mask=attention_mask, \n","                      start_positions=start_positions, \n","                      end_positions=end_positions,\n","                      is_impossibles=is_impossibles)\n","        \n","        \n","      loss = outputs[0]\n","      \n","      # F1 calculation\n","      f1 = compute_f1_batch(outputs, batch, tokenizer)\n","\n","      val_loss.append(loss)\n","      val_f1.append(f1)\n","    \n","      \n","\n","      loop.set_description(f'Epoch {epoch+1} Validation')\n","      loop.set_postfix(loss=loss.item(), F1=f1)\n","\n","  print(f'\\nEnd of epoch {epoch+1}|Training Loss: {sum(train_loss)/len(train_loss):.3f} F1 Score: \\\n","  {sum(train_f1)/len(train_f1):.3f}|Validation Loss: {sum(val_loss)/len(val_loss):.3f} \\\n","  F1 Score: {sum(val_f1)/len(val_f1):.3f}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3yDFvqmHIt_9"},"outputs":[],"source":["TOKENIZERS_PARALLELISM=False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b7zJjEYtIt_9"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}